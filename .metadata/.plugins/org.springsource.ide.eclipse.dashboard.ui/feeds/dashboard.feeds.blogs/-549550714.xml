<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Spring</title>
  <link rel="alternate" href="https://spring.io/blog" />
  <link rel="self" href="https://spring.io/blog.atom" />
  <id>http://spring.io/blog.atom</id>
  <icon>https://spring.io/favicon.ico</icon>
  <updated>2019-12-10T22:57:31Z</updated>
  <entry>
    <title>Spring Boot 2.1.x EOL November 1st 2020</title>
    <link rel="alternate" href="https://spring.io/blog/2019/12/10/spring-boot-2-1-x-eol-november-1st-2020" />
    <category term="releases" label="Releases" />
    <author>
      <name>Phil Webb</name>
    </author>
    <id>tag:spring.io,2019-12-10:3907</id>
    <updated>2019-12-10T22:57:31Z</updated>
    <content type="html">&lt;p&gt;With the recent release of Spring Boot 2.2, we&amp;rsquo;d like to announce that maintenance for Spring Boot 2.1 will end on November 1st 2020.&lt;/p&gt;
&lt;p&gt;We&amp;rsquo;ll keep publishing the occasional maintenance release up until that point, but we recommend that all users consider upgrading to Spring Boot 2.2 as soon as possible. Upgrading to Spring Boot 2.2 from 2.1 should not be too difficult, and upgrade instructions are &lt;a href="https://github.com/spring-projects/spring-boot/wiki/Spring-Boot-2.2-Release-Notes#upgrading-from-spring-boot-21"&gt;available on the WIKI&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In order to help track the state of supported releases we&amp;rsquo;ve also introduced a &lt;a href="https://github.com/spring-projects/spring-boot/wiki/Supported-Versions"&gt;new &amp;ldquo;supported versions&amp;rdquo; WIKI page&lt;/a&gt;. You can see at a glance which Spring Boot versions are supported and when they will EOL.&lt;/p&gt;
&lt;!-- rendered by Sagan Renderer Service --&gt;</content>
  </entry>
  <entry>
    <title>This Week in Spring - December 10th, 2019</title>
    <link rel="alternate" href="https://spring.io/blog/2019/12/10/this-week-in-spring-december-10th-2019" />
    <category term="engineering" label="Engineering" />
    <author>
      <name>Josh Long</name>
    </author>
    <id>tag:spring.io,2019-12-10:3906</id>
    <updated>2019-12-10T01:41:10Z</updated>
    <content type="html">&lt;p&gt;Hi, Spring fans! Welcome to another installment of &lt;em&gt;This Week in Spring&lt;/em&gt;! Today I just wrapped up my appearance in Brisbane, Australia, where I have been for the epic YOW! conference. Truly, one of my all-time favorite shows on the planet. I feel like an imposter in the ranks of the other speakers. I can not recommend this show enough.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;m just about to board my fight back to San Francisco, and we&amp;rsquo;ve got a ton of stuff to get to so let&amp;rsquo;s press on! &lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href="https://spring.io/blog/2019/12/09/spring-cloud-data-flow-2-3-0-ga-released"&gt;Spring Cloud Data Flow 2.3.0 GA Released&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href="https://spring.io/blog/2019/12/09/stream-processing-with-spring-cloud-stream-and-apache-kafka-streams-part-6-state-stores-and-interactive-queries"&gt;Stream Processing with Spring Cloud Stream and Apache Kafka Streams. Part 6 - State Stores and Interactive Queries&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href="https://spring.io/blog/2019/12/06/stream-processing-with-spring-cloud-stream-and-apache-kafka-streams-part-5-application-customizations"&gt;Stream Processing with Spring Cloud Stream and Apache Kafka Streams. Part 5 - Application Customizations&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href="https://spring.io/blog/2019/12/06/spring-data-r2dbc-goes-ga"&gt;Spring Data R2DBC goes GA&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href="https://spring.io/blog/2019/12/06/spring-boot-2-2-2-is-now-available"&gt;Spring Boot 2.2.2 is now available&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href="https://spring.io/blog/2019/12/06/spring-boot-2-1-11-is-now-available"&gt;Spring Boot 2.1.11 is now available&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;In last week&amp;rsquo;s &lt;em&gt;A Bootiful Podcast&lt;/em&gt;, I talked to &lt;a href="https://spring.io/blog/2019/12/05/a-bootiful-podcast-pivotal-s-katrina-bakas-about-the-pivotal-healthwatch-product-kubernetes-cloud-foundry-and-so-much-more"&gt;Pivotal&amp;rsquo;s Katrina Bakas about the Pivotal HealthWatch product, Kubernetes, Cloud Foundry and so much more.&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href="https://spring.io/blog/2019/12/05/stream-processing-with-spring-cloud-stream-and-apache-kafka-streams-part-4-error-handling"&gt;Stream Processing with Spring Cloud Stream and Apache Kafka Streams. Part 4 - Error Handling&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href="https://spring.io/blog/2019/12/04/stream-processing-with-spring-cloud-stream-and-apache-kafka-streams-part-3-data-deserialization-and-serialization"&gt;Stream Processing with Spring Cloud Stream and Apache Kafka Streams. Part 3 - Data deserialization and serialization&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href="https://spring.io/blog/2019/12/04/spring-data-moore-sr3-and-lovelace-sr14-released"&gt;Spring Data Moore SR3 and Lovelace SR14 released&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href="https://spring.io/blog/2019/12/03/stream-processing-with-spring-cloud-stream-and-apache-kafka-streams-part-2-programming-model-continued"&gt;Stream Processing with Spring Cloud Stream and Apache Kafka Streams. Part 2 - Programming Model Continued&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href="https://spring.io/blog/2019/12/03/spring-framework-maintenance-roadmap-in-2020-including-4-3-eol"&gt;Spring Framework maintenance roadmap in 2020 (including 4.3 EOL)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href="https://spring.io/blog/2019/12/03/spring-framework-5-2-2-and-5-1-12-available-now"&gt;Spring Framework 5.2.2 and 5.1.12 available now&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Reactor team member Sergei Egorov&amp;rsquo;s new &lt;a href="https://bsideup.github.io/posts/daily_reactive/where_is_my_exception/"&gt;&lt;em&gt;Daily Reactive&lt;/em&gt;&lt;/a&gt; series looks awesome! Well worth a read, too!&lt;/li&gt;
  &lt;li&gt;Have you checked out this month&amp;rsquo;s &lt;a href="https://www.rabbitmq.com/blog/2019/12/07/this-month-in-rabbitmq-november-2019-recap/"&gt;&lt;em&gt;This Month in RabbitMQ&lt;/em&gt; roundup yet?&lt;/a&gt;&lt;br/&gt;conference-recording-recommendations-2019/)&lt;/li&gt;
  &lt;li&gt;I love Trisha Gee¡¯s tutorial series introducing &lt;a href="https://blog.jetbrains.com/idea/tag/tutorial-reactive-spring/"&gt; reactive Spring Boot.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- rendered by Sagan Renderer Service --&gt;</content>
  </entry>
  <entry>
    <title>Spring Cloud Data Flow 2.3.0 GA Released</title>
    <link rel="alternate" href="https://spring.io/blog/2019/12/09/spring-cloud-data-flow-2-3-0-ga-released" />
    <category term="releases" label="Releases" />
    <author>
      <name>Janne Valkealahti</name>
    </author>
    <id>tag:spring.io,2019-12-06:3904</id>
    <updated>2019-12-09T19:16:48Z</updated>
    <content type="html">&lt;p&gt;The release 2.3.0 delivers a lot of enhancements and generic compatibility changes for Spring Boot 2.2.x and Spring Cloud Hoxton.&lt;/p&gt;&lt;h2&gt;&lt;a href="#continuous-deployment-for-tasks" class="anchor" name="continuous-deployment-for-tasks"&gt;&lt;/a&gt;Continuous Deployment For Tasks&lt;/h2&gt;
&lt;p&gt;As task applications evolve faster to keep up with business needs, the ability for new versions to be consumed via Data Flow in an automated way is needed. While Data Flow has supported the ability to register multiple versions of a task application in previous iterations, the ability to run them in a practical way by re-hydrating command line arguments, deployment properties, and application properties used in previous executions has been missing. In this version, the storage of those values in a manifest and the ability to both retrieve them to determine if an application needs to be upgraded and apply them to the new execution allows for developers to create continuous deployment flows for their task applications. All these capabilities are readily available through RESTful APIs, as well, so the overall CI/CD workflow for Tasks can be automated.&lt;/p&gt;&lt;h2&gt;&lt;a href="#scheduler-improvements" class="anchor" name="scheduler-improvements"&gt;&lt;/a&gt;Scheduler improvements&lt;/h2&gt;
&lt;p&gt;Scheduling has been updated to support the Task¡¯s CI/CD features. Thus when the Kubernetes or Cloud Foundry Task-scheduler launches the application after the user updates the Task application to a new version, the next execution in Spring Cloud Data Flow will take advantage of these updates automatically.&lt;/p&gt;&lt;h2&gt;&lt;a href="#monitoring-improvements" class="anchor" name="monitoring-improvements"&gt;&lt;/a&gt;Monitoring improvements&lt;/h2&gt;
&lt;p&gt;In this release, we have revised the monitoring architecture to drive towards a consistent experience in Local, Kubernetes, and Cloud Foundry.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Using &lt;a href="https://github.com/micrometer-metrics/prometheus-rsocket-proxy"&gt;Prometheus RSocket Proxy&lt;/a&gt; as a default approach for Prometheus-based monitoring of the &lt;a href="https://github.com/micrometer-metrics/prometheus-rsocket-proxy#support-for-short-lived-or-serverless-applications"&gt;short lived&lt;/a&gt; Tasks, as well as long-lived streaming applications, and across all supported platforms.&lt;/li&gt;
  &lt;li&gt;Native &lt;a href="https://dataflow.spring.io/docs/2.3.0.SNAPSHOT/feature-guides/batch/monitoring/"&gt;monitoring of Spring Cloud Tasks and Spring Cloud Batch&lt;/a&gt;, complements to the existing monitoring support for streaming applications through Spring Cloud Streams.&lt;/li&gt;
  &lt;li&gt;We have a few &lt;a href="https://github.com/spring-cloud/spring-cloud-dataflow-samples/tree/master/monitoring-samples"&gt;monitoring-samples&lt;/a&gt;. You will find instructions for building custom Stream and Task applications integrated with the Data Flow Monitoring Architecture:&lt;/li&gt;
  &lt;li&gt;&lt;a href="https://github.com/spring-cloud/spring-cloud-dataflow-samples/tree/master/monitoring-samples/stream-apps"&gt;stream-apps&lt;/a&gt; - how to enable monitoring for custom built source, processor and sink apps.&lt;/li&gt;
  &lt;li&gt;&lt;a href="https://github.com/spring-cloud/spring-cloud-dataflow-samples/tree/master/monitoring-samples/task-apps"&gt;task-apps&lt;/a&gt; - how to enable monitoring for custom built task apps.&lt;/li&gt;
  &lt;li&gt;Allow using the monitoring architecture to implement elastic, auto-scaling adapters for stream pipelines.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src="https://user-images.githubusercontent.com/50398/70357194-37365c00-186e-11ea-9bc2-5dfe4924d114.gif" alt="SCDF-monitoring-promethesu-proxy" /&gt;&lt;/p&gt;&lt;h2&gt;&lt;a href="#kubernetes-deployer-improvements" class="anchor" name="kubernetes-deployer-improvements"&gt;&lt;/a&gt;Kubernetes deployer improvements&lt;/h2&gt;
&lt;p&gt;The following new capabilities are readily available as &lt;a href="https://docs.spring.io/spring-cloud-dataflow/docs/2.3.0.RELEASE/reference/htmlsingle/#configuration-kubernetes-deployer"&gt;deployment properties&lt;/a&gt; in Kubrentes for both streaming and batch data pipelines.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Support for Node Affinity, Pod Affinity and Anti-Affinity&lt;/li&gt;
  &lt;li&gt;Ability to add multiple ports to Service objects&lt;/li&gt;
  &lt;li&gt;Allow customization of the container image used in StatefulSet deployments&lt;/li&gt;
  &lt;li&gt;Implementation of Scaling API&lt;/li&gt;
  &lt;li&gt;Support for custom init containers&lt;/li&gt;
&lt;/ul&gt;&lt;h2&gt;&lt;a href="#helm-chart-improvements" class="anchor" name="helm-chart-improvements"&gt;&lt;/a&gt;Helm chart improvements&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Ability to enable monitoring support using Prometheus and Grafana&lt;/li&gt;
  &lt;li&gt;Try out the &lt;a href="https://hub.helm.sh/charts/stable/spring-cloud-data-flow"&gt;2.3 GA compatible version of the Helm Chart&lt;/a&gt; and let us know what you think!&lt;/li&gt;
&lt;/ul&gt;&lt;h2&gt;&lt;a href="#scaling-api" class="anchor" name="scaling-api"&gt;&lt;/a&gt;Scaling API&lt;/h2&gt;
&lt;p&gt;The addition of new scaling API is available to quickly alter the number of application instances without redeploying a whole stream with updates to the deployment properties. The Scaling API is agnostic to the target platform and can be used seamlessly with K8s, CF and Local.&lt;/p&gt;&lt;h2&gt;&lt;a href="#import-export-utility" class="anchor" name="import-export-utility"&gt;&lt;/a&gt;Import/Export Utility&lt;/h2&gt;
&lt;p&gt;We also made it easier to work with multiple environments by adding &lt;a href="https://docs.spring.io/spring-cloud-dataflow/docs/2.3.0.RELEASE/reference/htmlsingle/#_import_export_streams"&gt;Import/Export Streams&lt;/a&gt; feature which provides easy moving streams across different environments, e.g. dev, test, prod.&lt;/p&gt;&lt;h2&gt;&lt;a href="#security" class="anchor" name="security"&gt;&lt;/a&gt;Security&lt;/h2&gt;
&lt;p&gt;Our journey to fully move into next generation Spring Security OAuth2 support is almost complete and we expect to finalize it in next releases. There is a blog post &lt;a href="https://spring.io/blog/2018/01/30/next-generation-oauth-2-0-support-with-spring-security"&gt;Next Generation OAuth 2.0 Support with Spring Security&lt;/a&gt; which outlined where things are going in a Spring world.&lt;/p&gt;&lt;h2&gt;&lt;a href="#developer-surveys" class="anchor" name="developer-surveys"&gt;&lt;/a&gt;Developer Surveys&lt;/h2&gt;
&lt;p&gt;We have released one major release (2.0), three minor releases (2.1, 2.2, and 2.3), and several maintenance releases in 2019! Likewise, Spring Cloud Stream, Spring Cloud Task, Deployers, Skipper, Apps, and remaining others in the SCDF ecosystem independently evolved in isolation, as well.&lt;/p&gt;
&lt;p&gt;Thank you, everyone, for your support, contributions, and participation!&lt;/p&gt;
&lt;p&gt;As we are inching towards the New Year¡¯s, we wanted to reach out to the community to learn about your interests and feedback. Please take these super quick 1-page surveys and let us know.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://forms.gle/VWjQai7DzBTFNVof6"&gt;Spring Cloud Data Flow&lt;/a&gt;&lt;br/&gt;&lt;a href="https://forms.gle/z9ja25wQrDZSmeQb7"&gt;Spring Cloud Stream&lt;/a&gt;&lt;br/&gt;&lt;a href="https://forms.gle/upYHVn3wVJnHKbyB8"&gt;Spring Cloud Task&lt;/a&gt;&lt;/p&gt;&lt;h2&gt;&lt;a href="#stay-in-touch-hellip" class="anchor" name="stay-in-touch-hellip"&gt;&lt;/a&gt;Stay in touch&amp;hellip;&lt;/h2&gt;
&lt;p&gt;As always, we welcome feedback and contributions, so please reach out to us on &lt;a href="https://stackoverflow.com/questions/tagged/spring-cloud-dataflow"&gt;Stackoverflow&lt;/a&gt; or &lt;a href="https://github.com/spring-cloud/spring-cloud-dataflow/issues"&gt;GitHub&lt;/a&gt; or via &lt;a href="https://gitter.im/spring-cloud/spring-cloud-dataflow"&gt;Gitter&lt;/a&gt;.&lt;/p&gt;
&lt;!-- rendered by Sagan Renderer Service --&gt;</content>
  </entry>
  <entry>
    <title>Stream Processing with Spring Cloud Stream and Apache Kafka Streams. Part 6 - State Stores and Interactive Queries</title>
    <link rel="alternate" href="https://spring.io/blog/2019/12/09/stream-processing-with-spring-cloud-stream-and-apache-kafka-streams-part-6-state-stores-and-interactive-queries" />
    <category term="engineering" label="Engineering" />
    <author>
      <name>Soby Chacko</name>
    </author>
    <id>tag:spring.io,2019-12-09:3905</id>
    <updated>2019-12-09T18:04:21Z</updated>
    <content type="html">&lt;p&gt;Part 1 - &lt;a href="https://spring.io/blog/2019/12/02/stream-processing-with-spring-cloud-stream-and-apache-kafka-streams-part-1-programming-model"&gt;Programming Model&lt;/a&gt;&lt;br/&gt;Part 2 - &lt;a href="https://spring.io/blog/2019/12/03/stream-processing-with-spring-cloud-stream-and-apache-kafka-streams-part-2-programming-model-continued"&gt;Programming Model Continued&lt;/a&gt;&lt;br/&gt;Part 3 - &lt;a href="https://spring.io/blog/2019/12/04/stream-processing-with-spring-cloud-stream-and-apache-kafka-streams-part-3-data-deserialization-and-serialization"&gt;Data deserialization and serialization&lt;/a&gt;&lt;br/&gt;Part 4 - &lt;a href="https://spring.io/blog/2019/12/05/stream-processing-with-spring-cloud-stream-and-apache-kafka-streams-part-4-error-handling"&gt;Error Handling&lt;/a&gt;&lt;br/&gt;Part 5 - &lt;a href="https://spring.io/blog/2019/12/06/stream-processing-with-spring-cloud-stream-and-apache-kafka-streams-part-5-application-customizations"&gt;Application Customizations&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In this part (the sixth and final one of this series), we are going to look into the ways Spring Cloud Stream Binder for Kafka Streams supports state stores and interactive queries in Kafka Streams.&lt;/p&gt;&lt;h2&gt;&lt;a href="#named-state-stores" class="anchor" name="named-state-stores"&gt;&lt;/a&gt;Named State Stores&lt;/h2&gt;
&lt;p&gt;When you have the need to maintain state in the application, Kafka Streams lets you materialize that state information into a named state store. There are several operations in Kafka Streams that require it to keep track of the state such as &lt;code&gt;count&lt;/code&gt;, &lt;code&gt;aggregate&lt;/code&gt;, &lt;code&gt;reduce&lt;/code&gt;, various &lt;code&gt;windowing&lt;/code&gt; operations, and others. Kafka Streams uses a special database called &lt;a href="https://rocksdb.org/"&gt;RocksDB&lt;/a&gt; for maintaining this state store in most cases (unless you explicitly change the store type). By default, the same information in the state store is backed up to a changelog topic as well as within Kafka, for fault-tolerant reasons. &lt;/p&gt;
&lt;p&gt;When you explicitly materialize state like this into a named state store, this gives the ability for applications to query that state store at a later stage. This is a very powerful feature, as this gives you the ability to query into a database-like structure from within your Kafka Streams applications.&lt;/p&gt;&lt;h2&gt;&lt;a href="#consuming-data-as-ktable-or-globalktable" class="anchor" name="consuming-data-as-ktable-or-globalktable"&gt;&lt;/a&gt;Consuming data as KTable or GlobalKTable&lt;/h2&gt;
&lt;p&gt;Kafka Streams binder-based applications can bind to destinations as &lt;code&gt;KTable&lt;/code&gt; or &lt;code&gt;GlobalKTable&lt;/code&gt;. &lt;code&gt;GlobalKTable&lt;/code&gt; is a special table type, where you get data from all partitions of an input topic, regardless of the instance that it is running. By contrast, a &lt;code&gt;KTable&lt;/code&gt; gives you only data from the respective partitions of the topic that the instance is consuming from. &lt;/p&gt;
&lt;p&gt;The following is a function signature we saw earlier in this series of blog posts:&lt;/p&gt;
&lt;pre&gt;&lt;code class="prettyprint"&gt;@Bean&#xD;
public Function&amp;lt;KStream&amp;lt;Long, Order&amp;gt;,&#xD;
     Function&amp;lt;KTable&amp;lt;Long, Customer&amp;gt;,&#xD;
           Function&amp;lt;GlobalKTable&amp;lt;Long, Product&amp;gt;, KStream&amp;lt;Long, EnrichedOrder&amp;gt;&amp;gt;&amp;gt;&amp;gt; process() {
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As you can see, this function has three input bindings, one &lt;code&gt;KStream&lt;/code&gt;, one &lt;code&gt;KTable&lt;/code&gt;, and another &lt;code&gt;GlobalKTable&lt;/code&gt;. Kafka Streams lets you materialize tables consumed like these into named state stores, given that these tables are based on a primary key. You can use the binding level property to materialize them into named state stores along with consumption. The following examples show how to do so:&lt;/p&gt;
&lt;pre&gt;&lt;code class="prettyprint"&gt;spring.cloud.stream.kafka.streams.bindings.process-in-1.consumer.materializedAs: incoming-store-1&#xD;
spring.cloud.stream.kafka.streams.bindings.process-in-2.consumer.materializedAs: incoming-store-2
&lt;/code&gt;&lt;/pre&gt;&lt;h2&gt;&lt;a href="#kafka-streams-dsl-operations-materialized-into-state-stores" class="anchor" name="kafka-streams-dsl-operations-materialized-into-state-stores"&gt;&lt;/a&gt;Kafka Streams DSL operations materialized into state stores&lt;/h2&gt;
&lt;p&gt;There are various methods in the Kafka Streams high-level DSL, which returns table types such as &lt;code&gt;count&lt;/code&gt;, &lt;code&gt;aggregate&lt;/code&gt;, and &lt;code&gt;reduce&lt;/code&gt;. There are other operations that use state stores to keep track of information. For example, the various join method calls in &lt;code&gt;KStream&lt;/code&gt;, although they return a &lt;code&gt;KStream&lt;/code&gt; type, internally use state stores to keep the joined data. In summary, when Kafka Streams lets you materialize data either as a table or stream, it is materialized into a state store, much like data stored in a database table. &lt;/p&gt;&lt;h2&gt;&lt;a href="#explicit-state-stores-to-use-in-low-level-processors" class="anchor" name="explicit-state-stores-to-use-in-low-level-processors"&gt;&lt;/a&gt;Explicit state stores to use in low-level processors&lt;/h2&gt;
&lt;p&gt;When using the processor API of Kafka Streams, which gives you more flexibility on how the stream is processed, you have to declare a state store beforehand and provide that to the StreamsBuilder. Kafka Streams binder can scan the application to detect beans of type StoreBuilder and then use that to create state stores and pass them along with the underlying StreamsBuilder through the StreamsBuilderFactoryBean. Here is a look at such beans:&lt;/p&gt;
&lt;pre&gt;&lt;code class="prettyprint"&gt;@Bean&#xD;
public StoreBuilder myStore() {&#xD;
  return Stores.keyValueStoreBuilder(&#xD;
        Stores.persistentKeyValueStore(&amp;quot;my-store&amp;quot;), Serdes.Long(),&#xD;
        Serdes.Long());&#xD;
}&#xD;
&#xD;
@Bean&#xD;
public StoreBuilder otherStore() {&#xD;
  return Stores.windowStoreBuilder(&#xD;
        Stores.persistentWindowStore(&amp;quot;other-store&amp;quot;,&#xD;
              Duration.ofSeconds(3), Duration.ofSeconds(3),  false), Serdes.Long(),&#xD;
        Serdes.Long());&#xD;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The two StoreBuilder beans are detected by the binder, and it then attaches them to the streams builder automatically. Later on, you can access them, in your processor API based applications, as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code class="prettyprint"&gt;¡¦&#xD;
KeyValueStore&amp;lt;Long, Long&amp;gt; state1;&#xD;
WindowStore&amp;lt;Long, Long&amp;gt; state2;&#xD;
...&#xD;
@Bean&#xD;
public java.util.function.Consumer&amp;lt;KStream&amp;lt;Object, String&amp;gt;&amp;gt; process() {&#xD;
  return input -&amp;gt;&#xD;
        input.process((ProcessorSupplier&amp;lt;Object, String&amp;gt;) () -&amp;gt; new Processor&amp;lt;Object, String&amp;gt;() {&#xD;
           @Override&#xD;
            public void init(ProcessorContext context) {&#xD;
              state1 = (KeyValueStore&amp;lt;Long, Long&amp;gt;) context.getStateStore(&amp;quot;my-store&amp;quot;);&#xD;
              state2 = (WindowStore&amp;lt;Long, Long&amp;gt;) context.getStateStore(&amp;quot;other-store&amp;quot;);&#xD;
           }&#xD;
&#xD;
           @Override&#xD;
           public void process(Object key, String value) {&#xD;
              // processing code&#xD;
           }&#xD;
&#xD;
           @Override&#xD;
           public void close() {&#xD;
              if (state1 != null) {&#xD;
                 state1.close();&#xD;
              }&#xD;
              if (state2 != null) {&#xD;
                 state2.close();&#xD;
              }&#xD;
           }&#xD;
        }, &amp;quot;my-store&amp;quot;, &amp;quot;other-store&amp;quot;);&#xD;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;One quick note about the usage of the processor API in Kafka Streams binder-based applications. The only way you can use the low-level processor API when you use the binder is through a usage pattern of higher-level DSL and then combine that with a transform or process call on it, as shown in the preceding example. See &lt;a href="https://cloud.spring.io/spring-cloud-static/spring-cloud-stream-binder-kafka/3.0.0.RELEASE/reference/html/spring-cloud-stream-binder-kafka.html#_mixing_high_level_dsl_and_low_level_processor_api"&gt;here&lt;/a&gt; for more details on how the processor API can be used in a binder based application. &lt;/p&gt;
&lt;p&gt;Instead of creating &lt;code&gt;StoreBuilder&lt;/code&gt; beans in the application, you can also use the &lt;code&gt;StreamsBuilderFactoryBean&lt;/code&gt; customizer which we saw in the &lt;a href="https://spring.io/blog/2019/12/06/stream-processing-with-spring-cloud-stream-and-apache-kafka-streams-part-5-application-customizations"&gt;previous blog&lt;/a&gt;, to add the state stores programmatically, if that is your preference. &lt;/p&gt;&lt;h2&gt;&lt;a href="#using-interactive-queries-to-query-data-from-state-stores" class="anchor" name="using-interactive-queries-to-query-data-from-state-stores"&gt;&lt;/a&gt;Using interactive queries to query data from state stores&lt;/h2&gt;
&lt;p&gt;Kafka Streams lets you interactively query the data in the state store in real time as live stream processing is going on. The binder provides abstractions around this feature to make it easier to work with interactive queries. &lt;code&gt;InteractiveQueryService&lt;/code&gt; is a basic API that the binder provides to work with state store querying. You can usually inject this as a bean into your application and then invoke various API methods from it. Here is an example:&lt;/p&gt;
&lt;pre&gt;&lt;code class="prettyprint"&gt;@Autowired&#xD;
private InteractiveQueryService interactiveQueryService;&#xD;
 ¡¦&#xD;
 ...&#xD;
ReadOnlyKeyValueStore&amp;lt;Object, Object&amp;gt; keyValueStore =&#xD;
                                                interactiveQueryService.getQueryableStoreType(&amp;quot;my-store&amp;quot;, QueryableStoreTypes.keyValueStore());
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then you can invoke various retrieval methods from the store and iterate through the result. There are various methods that you can invoke from these state stores based on your use case and the type of state store that you are using. Please refer to the Kafka Streams documentation for &lt;a href="https://kafka.apache.org/10/documentation/streams/developer-guide/interactive-queries.html"&gt;interactive queries&lt;/a&gt; for these various iteration methods available. &lt;/p&gt;&lt;h2&gt;&lt;a href="#interactive-queries-over-rpc-mechanisms" class="anchor" name="interactive-queries-over-rpc-mechanisms"&gt;&lt;/a&gt;Interactive Queries over RPC Mechanisms&lt;/h2&gt;
&lt;p&gt;Oftentimes, you want to expose the state of your system from state stores over an RPC mechanism. You can combine Spring web support for writing powerful REST based applications in this manner. Here is a blueprint:&lt;/p&gt;
&lt;pre&gt;&lt;code class="prettyprint"&gt;@RestController&#xD;
public class Controller {&#xD;
&#xD;
		@RequestMapping(&amp;quot;/song/id&amp;quot;)&#xD;
		public SongBean song(@RequestParam(value=&amp;quot;id&amp;quot;) Long id) {&#xD;
			final ReadOnlyKeyValueStore&amp;lt;Long, Song&amp;gt; songStore =&#xD;
					interactiveQueryService.getQueryableStore(¡°song-store¡±, QueryableStoreTypes.&amp;lt;Long, Song&amp;gt;keyValueStore());&#xD;
&#xD;
			final Song song = songStore.get(id);&#xD;
			if (song == null) {&#xD;
				throw new IllegalArgumentException(&amp;quot;...&amp;quot;);&#xD;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This REST controller can be accessed from a front end web application for example. &lt;/p&gt;
&lt;p&gt;This usage pattern obviously raises concerns. What happens if there are multiple Kafka Streams application instances running? For instance, what if there are 3 instances in which each of them is pulling data from a single source partition? Which controller instance is going to be responsible for providing information for key &lt;em&gt;X&lt;/em&gt;? What if key &lt;em&gt;X&lt;/em&gt; is only hosted in partition 3 and that happens to be the instance 3, but the request landed on instance 1. This is obviously a problem, but Kafka Streams provides a solution for that. &lt;/p&gt;&lt;h2&gt;&lt;a href="#retrieving-a-key-from-the-right-instance" class="anchor" name="retrieving-a-key-from-the-right-instance"&gt;&lt;/a&gt;Retrieving a key from the right instance&lt;/h2&gt;
&lt;p&gt;When you have multiple instances running and you want to use interactive queries, you have to set this property at the binder level:&lt;/p&gt;
&lt;pre&gt;&lt;code class="prettyprint"&gt;spring.cloud.stream.kafka.streams.binder.configuration.application.server: &amp;lt;server&amp;gt;:&amp;lt;port&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, in the controller method, you have to write logic that is similar to the following:&lt;/p&gt;
&lt;pre&gt;&lt;code class="prettyprint"&gt;@RequestMapping(&amp;quot;/charts/top-five&amp;quot;)&#xD;
@SuppressWarnings(&amp;quot;unchecked&amp;quot;)&#xD;
public List&amp;lt;SongPlayCountBean&amp;gt; topFive(@RequestParam(value=&amp;quot;genre&amp;quot;) String genre) {&#xD;
{&#xD;
&#xD;
org.apache.kafka.streams.state.HostInfo hostInfo = interactiveQueryService.getHostInfo(&amp;quot;store-name&amp;quot;,&#xD;
                                                key, keySerializer);&#xD;
&#xD;
if (interactiveQueryService.getCurrentHostInfo().equals(hostInfo)) {&#xD;
&#xD;
    //query from the store that is locally available&#xD;
}&#xD;
else {&#xD;
    //query from the remote host&#xD;
RestTemplate restTemplate = new RestTemplate();&#xD;
	return restTemplate.postForObject(&#xD;
						String.format(&amp;quot;http://%s:%d/%s&amp;quot;, hostInfo.host(),&#xD;
								hostInfo.port(), &amp;quot;charts/top-five?genre=Punk&amp;quot;), ¡¦);&#xD;
&#xD;
}
&lt;/code&gt;&lt;/pre&gt;&lt;h2&gt;&lt;a href="#summary" class="anchor" name="summary"&gt;&lt;/a&gt;Summary&lt;/h2&gt;
&lt;p&gt;In this blog, we saw the various ways in which Kafka Streams lets you materialize state information into state stores. The binder lets you consume data as &lt;code&gt;KTable&lt;/code&gt; or &lt;code&gt;GlobalKTable&lt;/code&gt; while allowing you to materialize that into a named state store. Kafka Streams has several operations in which state stores can be materialized as named stores. We saw that, when using the processor API in Kafka Streams, the application needs to create state store builder beans that the binder detects which it then passes along to Kafka Streams. Finally, we saw how these state stores can be queried by using interactive queries. We also saw the nuances involving multiple instances of an application and interactive queries against them. &lt;/p&gt;&lt;h2&gt;&lt;a href="#concluding-the-series-and-where-to-go-next-hellip" class="anchor" name="concluding-the-series-and-where-to-go-next-hellip"&gt;&lt;/a&gt;Concluding the series and Where to Go Next&amp;hellip;&lt;/h2&gt;
&lt;p&gt;Thank you for reading this blog series!&lt;/p&gt;
&lt;p&gt;In this six-part series, we saw many features of Kafka Streams binder in Spring Cloud Stream, such as its &lt;a href="https://spring.io/blog/2019/12/03/stream-processing-with-spring-cloud-stream-and-apache-kafka-streams-part-2-programming-model-continued"&gt;programming models&lt;/a&gt;, &lt;a href="https://spring.io/blog/2019/12/04/stream-processing-with-spring-cloud-stream-and-apache-kafka-streams-part-3-data-deserialization-and-serialization"&gt;data serialization&lt;/a&gt;, &lt;a href="https://spring.io/blog/2019/12/05/stream-processing-with-spring-cloud-stream-and-apache-kafka-streams-part-4-error-handling"&gt;error handling&lt;/a&gt;, &lt;a href="https://spring.io/blog/2019/12/06/stream-processing-with-spring-cloud-stream-and-apache-kafka-streams-part-5-application-customizations"&gt;customization&lt;/a&gt;, and interactively querying the state stores. There are more features that we haven¡¯t covered as part of this series as we wanted to focus on the general theme of introducing the main features of this binder that was added or enhanced in version &lt;code&gt;3.0.0&lt;/code&gt;. For those additional features or to engage with the engineering team behind Spring Cloud Stream, please check out the various links provided in the resources section below.&lt;/p&gt;&lt;h2&gt;&lt;a href="#resources" class="anchor" name="resources"&gt;&lt;/a&gt;Resources:&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://cloud.spring.io/spring-cloud-static/spring-cloud-stream-binder-kafka/3.0.0.RELEASE/reference/html/spring-cloud-stream-binder-kafka.html#_kafka_streams_binder"&gt;Kafka Streams Binder Docs&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://spring.io/projects/spring-cloud-stream"&gt;Spring Cloud Stream&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/spring-cloud/spring-cloud-stream"&gt;Core Spring Cloud Stream GitHub&lt;/a&gt;&lt;br/&gt;&lt;a href="https://github.com/spring-cloud/spring-cloud-stream-binder-kafka"&gt;Spring Cloud Stream Kafka Binder GitHub&lt;/a&gt;&lt;br/&gt;&lt;a href="https://github.com/spring-cloud/spring-cloud-stream-samples"&gt;Spring Cloud Stream Samples&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://stackoverflow.com/questions/tagged/spring-cloud-stream"&gt;Stack Overflow&lt;/a&gt;&lt;br/&gt;&lt;a href="https://gitter.im/spring-cloud/spring-cloud-stream"&gt;Gitter&lt;/a&gt;&lt;/p&gt;
&lt;!-- rendered by Sagan Renderer Service --&gt;</content>
  </entry>
  <entry>
    <title>Stream Processing with Spring Cloud Stream and Apache Kafka Streams. Part 5 - Application Customizations</title>
    <link rel="alternate" href="https://spring.io/blog/2019/12/06/stream-processing-with-spring-cloud-stream-and-apache-kafka-streams-part-5-application-customizations" />
    <category term="engineering" label="Engineering" />
    <author>
      <name>Soby Chacko</name>
    </author>
    <id>tag:spring.io,2019-12-06:3903</id>
    <updated>2019-12-06T16:58:00Z</updated>
    <content type="html">&lt;p&gt;Part 1 - &lt;a href="https://spring.io/blog/2019/12/02/stream-processing-with-spring-cloud-stream-and-apache-kafka-streams-part-1-programming-model"&gt;Programming Model&lt;/a&gt;&lt;br/&gt;Part 2 - &lt;a href="https://spring.io/blog/2019/12/03/stream-processing-with-spring-cloud-stream-and-apache-kafka-streams-part-2-programming-model-continued"&gt;Programming Model Continued&lt;/a&gt;&lt;br/&gt;Part 3 - &lt;a href="https://spring.io/blog/2019/12/04/stream-processing-with-spring-cloud-stream-and-apache-kafka-streams-part-3-data-deserialization-and-serialization"&gt;Data deserialization and serialization&lt;/a&gt;&lt;br/&gt;Part 4 - &lt;a href="https://spring.io/blog/2019/12/05/stream-processing-with-spring-cloud-stream-and-apache-kafka-streams-part-4-error-handling"&gt;Error Handling&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In this blog post, we continue our discussion on the support for Kafka Streams in Spring Cloud Stream. We are going to elaborate on the ways in which you can customize a Kafka Streams application. &lt;/p&gt;&lt;h2&gt;&lt;a href="#customizing-the-streamsbuilderfactorybean" class="anchor" name="customizing-the-streamsbuilderfactorybean"&gt;&lt;/a&gt;Customizing the StreamsBuilderFactoryBean&lt;/h2&gt;
&lt;p&gt;Kafka Streams binder uses the &lt;a href="https://docs.spring.io/spring-kafka/docs/current/api/org/springframework/kafka/config/StreamsBuilderFactoryBean.html"&gt;StreamsBuilderFactoryBean&lt;/a&gt;, provided by the &lt;a href="https://spring.io/projects/spring-kafka"&gt;Spring for Apache Kafka&lt;/a&gt; project, to build the &lt;a href="https://kafka.apache.org/23/javadoc/org/apache/kafka/streams/StreamsBuilder.html"&gt;StreamsBuilder&lt;/a&gt; object that is the foundation for a Kafka Streams application. This factory bean is a Spring lifecycle bean. Oftentimes, this factory bean must be customized before it is started, for various reasons. As described in the &lt;a href="https://spring.io/blog/2019/12/05/stream-processing-with-spring-cloud-stream-and-apache-kafka-streams-part-4-error-handling"&gt;previous blog&lt;/a&gt; post on error handling, you need to customize the &lt;code&gt;StreamsBuilderFactoryBean&lt;/code&gt; if you want to register a production exception handler. Let¡¯s say that you have this producer exception handler:&lt;/p&gt;
&lt;pre&gt;&lt;code class="prettyprint"&gt;class IgnoreRecordTooLargeHandler implements ProductionExceptionHandler {&#xD;
    &#xD;
    public ProductionExceptionHandlerResponse handle(final ProducerRecord&amp;lt;byte[], byte[]&amp;gt; record,&#xD;
                                                     final Exception exception) {&#xD;
        if (exception instanceof RecordTooLargeException) {&#xD;
            return ProductionExceptionHandlerResponse.CONTINUE;&#xD;
        } else {&#xD;
            return ProductionExceptionHandlerResponse.FAIL;&#xD;
        }&#xD;
    }&#xD;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can register it directly by using configuration if you choose (using &lt;code&gt;default.production.exception.handler&lt;/code&gt;). &lt;/p&gt;
&lt;p&gt;However, a more elegant approach, when using the binder, is to register this as part of the &lt;code&gt;StreamsBuilderFactoryBean&lt;/code&gt; customizer, as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code class="prettyprint"&gt;@Bean&#xD;
public StreamsBuilderFactoryBeanCustomizer customizer() {&#xD;
    return fb -&amp;gt; {&#xD;
        fb.getStreamsConfiguration().put(StreamsConfig.DEFAULT_PRODUCTION_EXCEPTION_HANDLER_CLASS_CONFIG,&#xD;
                            IgnoreRecordTooLargeHandler.class);&#xD;
    };&#xD;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that, if you have multiple processors in the application, you can control which processor gets customization based on the application ID. For example, you can check on it this way:&lt;/p&gt;
&lt;pre&gt;&lt;code class="prettyprint"&gt;return factoryBean -&amp;gt; {&#xD;
        if (factoryBean.getStreamsConfiguration().getProperty(StreamsConfig.APPLICATION_ID_CONFIG)&#xD;
                .equals(&amp;quot;processor1-application-id&amp;quot;)) {
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here is another example of setting a state listener:&lt;/p&gt;
&lt;pre&gt;&lt;code class="prettyprint"&gt;@Bean&#xD;
public StreamsBuilderFactoryBeanCustomizer streamsBuilderFactoryBeanCustomizer() {&#xD;
    return sfb -&amp;gt; sfb.setStateListener((newState, oldState) -&amp;gt; {&#xD;
         //Do some action here!&#xD;
    });&#xD;
}
&lt;/code&gt;&lt;/pre&gt;&lt;h2&gt;&lt;a href="#customizing-kafkastreams-object" class="anchor" name="customizing-kafkastreams-object"&gt;&lt;/a&gt;Customizing KafkaStreams Object.&lt;/h2&gt;
&lt;p&gt;The &lt;a href="https://kafka.apache.org/23/javadoc/org/apache/kafka/streams/KafkaStreams.html"&gt;KafkaStreams&lt;/a&gt; object is at the heart of any Kafka Streams application. &lt;code&gt;StreamsBuilderFactoryBean&lt;/code&gt; is responsible for creating the topology and then creating the &lt;code&gt;KafkaStreams&lt;/code&gt; object. Before starting the &lt;code&gt;KafkaStreams&lt;/code&gt; object, StreamsBuilderFactoryBean gives an opportunity to customize this &lt;code&gt;KafkaStreams&lt;/code&gt; object. For example, if you want to set an application-wide handler for uncaught exceptions you can do the following:&lt;/p&gt;
&lt;pre&gt;&lt;code class="prettyprint"&gt;@Bean&#xD;
public StreamsBuilderFactoryBeanCustomizer streamsBuilderFactoryBeanCustomizer() {&#xD;
    return factoryBean -&amp;gt; {&#xD;
        factoryBean.setKafkaStreamsCustomizer(new KafkaStreamsCustomizer() {&#xD;
            @Override&#xD;
            public void customize(KafkaStreams kafkaStreams) {&#xD;
                kafkaStreams.setUncaughtExceptionHandler((t, e) -&amp;gt; {&#xD;
&#xD;
                });&#xD;
            }&#xD;
        });&#xD;
    };&#xD;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice that we start with the customizer for StreamsBuilderFactoryBean. However, inside it, we use a separate &lt;code&gt;KafkaStreamsCustomizer&lt;/code&gt;.&lt;/p&gt;&lt;h2&gt;&lt;a href="#summary" class="anchor" name="summary"&gt;&lt;/a&gt;Summary&lt;/h2&gt;
&lt;p&gt;In this blog post, we saw how the Kafka Streams binder in Spring Cloud Stream lets you customize the underlying &lt;code&gt;StreamsBuilderFactoryBean&lt;/code&gt; and the &lt;code&gt;KafkaStreams&lt;/code&gt; object. &lt;/p&gt;
&lt;p&gt;Thank you for reading this far! Next, in the final blog post in this series, we will look at how the binder lets you deal with state stores and enabling interactive queries against them. &lt;/p&gt;
&lt;!-- rendered by Sagan Renderer Service --&gt;</content>
  </entry>
  <entry>
    <title>Spring Data R2DBC goes GA</title>
    <link rel="alternate" href="https://spring.io/blog/2019/12/06/spring-data-r2dbc-goes-ga" />
    <category term="releases" label="Releases" />
    <author>
      <name>Mark Paluch</name>
    </author>
    <id>tag:spring.io,2019-12-06:3902</id>
    <updated>2019-12-06T13:06:00Z</updated>
    <content type="html">&lt;p&gt;On behalf of the team and everyone that contributed, I am delighted to announce that Spring Data R2DBC 1.0 is generally available from &lt;a href="https://repo.spring.io/"&gt;repo.spring.io&lt;/a&gt; as well as Maven Central! &lt;/p&gt;
&lt;p&gt;Spring Data R2DBC 1.0 is a non-blocking database client library for the &lt;a href="https://r2dbc.io/2019/12/02/r2dbc-0-8-0-goes-ga"&gt;just released R2DBC specification&lt;/a&gt; that lets you build reactive applications that use SQL databases. The most notable features of Spring Data R2DBC are:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Functional-reactive declaration of data access&lt;/li&gt;
  &lt;li&gt;Fluent API&lt;/li&gt;
  &lt;li&gt;Support for Transactions&lt;/li&gt;
  &lt;li&gt;Named parameter support (Dialect-aware)&lt;/li&gt;
  &lt;li&gt;Repositories&lt;/li&gt;
  &lt;li&gt;Kotlin Coroutines extensions&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Spring Data R2DBC 1.0 requires JDK 8 or higher and &lt;a href="https://r2dbc.io/drivers/"&gt;any R2DBC driver&lt;/a&gt;. Head over to &lt;a href="https://start.spring.io"&gt;start.spring.io&lt;/a&gt; and add R2DBC to configure your dependencies or, if you&amp;rsquo;re already using the Spring Boot R2DBC starter, upgrade your &lt;code&gt;spring-boot-bom-r2dbc&lt;/code&gt; to &lt;code&gt;0.1.0.M3&lt;/code&gt;. &lt;/p&gt;
&lt;p&gt;Check out our byte-sized Getting Started Guide that explains &lt;a href="https://spring.io/guides/gs/accessing-data-r2dbc/"&gt;how to access data with R2DBC&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To round things off, here are links to the changelog, GitHub repository, and docs:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Project Repository: &lt;a href="https://github.com/spring-projects/spring-data-r2dbc"&gt;github.com/spring-projects/spring-data-r2dbc&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Issue Tracker: &lt;a href="https://github.com/spring-projects/spring-data-r2dbc/issues"&gt;github.com/spring-projects/spring-data-r2dbc/issues&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href="https://repo1.maven.org/maven2/org/springframework/data/spring-data-r2dbc/1.0.0.RELEASE/"&gt;Artifacts&lt;/a&gt; - &lt;a href="https://docs.spring.io/spring-data/r2dbc/docs/1.0.0.RELEASE/api"&gt;Javadoc&lt;/a&gt; - &lt;a href="https://docs.spring.io/spring-data/r2dbc/docs/1.0.0.RELEASE/reference/html/"&gt;Documentation&lt;/a&gt; - &lt;a href="https://docs.spring.io/spring-data/r2dbc/docs/1.0.0.RELEASE/changelog.txt"&gt;Changelog&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- rendered by Sagan Renderer Service --&gt;</content>
  </entry>
  <entry>
    <title>Spring Boot 2.2.2 is now available</title>
    <link rel="alternate" href="https://spring.io/blog/2019/12/06/spring-boot-2-2-2-is-now-available" />
    <category term="releases" label="Releases" />
    <author>
      <name>Brian Clozel</name>
    </author>
    <id>tag:spring.io,2019-12-06:3901</id>
    <updated>2019-12-06T10:50:00Z</updated>
    <content type="html">&lt;p&gt;On behalf of the team and everyone who has contributed, I&amp;rsquo;m happy to announce that Spring Boot 2.2.2 has been released and is now available from &lt;a href="https://repo.spring.io/release"&gt;repo.spring.io&lt;/a&gt; and Maven Central.&lt;/p&gt;
&lt;p&gt;This release includes &lt;a href="https://github.com/spring-projects/spring-boot/releases/tag/v2.2.2.RELEASE"&gt;88 fixes, improvements, and dependency upgrades&lt;/a&gt;. Thanks to all those who have contributed with issue reports and pull requests.&lt;/p&gt;&lt;h3&gt;&lt;a href="#how-can-you-help" class="anchor" name="how-can-you-help"&gt;&lt;/a&gt;How can you help?&lt;/h3&gt;
&lt;p&gt;If you&amp;rsquo;re interested in helping out, check out the &lt;a href="https://github.com/spring-projects/spring-boot/labels/status%3A%20ideal-for-contribution"&gt;&amp;ldquo;ideal for contribution&amp;rdquo; tag&lt;/a&gt; in the issue repository. If you have general questions, please ask on &lt;a href="http://stackoverflow.com"&gt;stackoverflow.com&lt;/a&gt; using the &lt;a href="http://stackoverflow.com/tags/spring-boot"&gt;&lt;code&gt;spring-boot&lt;/code&gt; tag&lt;/a&gt; or chat with the community on &lt;a href="https://gitter.im/spring-projects/spring-boot"&gt;Gitter&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://spring.io/projects/spring-boot/"&gt;Project Page&lt;/a&gt; | &lt;a href="https://github.com/spring-projects/spring-boot"&gt;GitHub&lt;/a&gt; | &lt;a href="https://github.com/spring-projects/spring-boot/issues"&gt;Issues&lt;/a&gt; | &lt;a href="http://docs.spring.io/spring-boot/docs/2.2.2.RELEASE/reference/html"&gt;Documentation&lt;/a&gt; | &lt;a href="http://stackoverflow.com/questions/tagged/spring-boot"&gt;Stack Overflow&lt;/a&gt; | &lt;a href="https://gitter.im/spring-projects/spring-boot"&gt;Gitter&lt;/a&gt;&lt;/p&gt;
&lt;!-- rendered by Sagan Renderer Service --&gt;</content>
  </entry>
  <entry>
    <title>Spring Boot 2.1.11 is now available</title>
    <link rel="alternate" href="https://spring.io/blog/2019/12/06/spring-boot-2-1-11-is-now-available" />
    <category term="releases" label="Releases" />
    <author>
      <name>Madhura Bhave</name>
    </author>
    <id>tag:spring.io,2019-12-05:3899</id>
    <updated>2019-12-06T03:22:00Z</updated>
    <content type="html">&lt;p&gt;On behalf of the team and everyone who has contributed, I&amp;rsquo;m happy to announce that Spring Boot 2.1.11 has been released and is now available from &lt;a href="https://repo.spring.io/release"&gt;repo.spring.io&lt;/a&gt; and Maven Central.&lt;/p&gt;
&lt;p&gt;This release includes &lt;a href="https://github.com/spring-projects/spring-boot/releases/tag/v2.1.11.RELEASE"&gt;53 fixes, improvements, and dependency upgrades&lt;/a&gt;. Thanks to all those who have contributed with issue reports and pull requests.&lt;/p&gt;&lt;h3&gt;&lt;a href="#how-can-you-help" class="anchor" name="how-can-you-help"&gt;&lt;/a&gt;How can you help?&lt;/h3&gt;
&lt;p&gt;If you&amp;rsquo;re interested in helping out, check out the &lt;a href="https://github.com/spring-projects/spring-boot/labels/status%3A%20ideal-for-contribution"&gt;&amp;ldquo;ideal for contribution&amp;rdquo; tag&lt;/a&gt; in the issue repository. If you have general questions, please ask on &lt;a href="http://stackoverflow.com"&gt;stackoverflow.com&lt;/a&gt; using the &lt;a href="http://stackoverflow.com/tags/spring-boot"&gt;&lt;code&gt;spring-boot&lt;/code&gt; tag&lt;/a&gt; or chat with the community on &lt;a href="https://gitter.im/spring-projects/spring-boot"&gt;Gitter&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://spring.io/projects/spring-boot/"&gt;Project Page&lt;/a&gt; | &lt;a href="https://github.com/spring-projects/spring-boot"&gt;GitHub&lt;/a&gt; | &lt;a href="https://github.com/spring-projects/spring-boot/issues"&gt;Issues&lt;/a&gt; | &lt;a href="http://docs.spring.io/spring-boot/docs/2.1.11.RELEASE/reference/html"&gt;Documentation&lt;/a&gt; | &lt;a href="http://stackoverflow.com/questions/tagged/spring-boot"&gt;Stack Overflow&lt;/a&gt; | &lt;a href="https://gitter.im/spring-projects/spring-boot"&gt;Gitter&lt;/a&gt;&lt;/p&gt;
&lt;!-- rendered by Sagan Renderer Service --&gt;</content>
  </entry>
  <entry>
    <title>A Bootiful Podcast: Pivotal's Katrina Bakas about the Pivotal HealthWatch product, Kubernetes, Cloud Foundry and so much more.</title>
    <link rel="alternate" href="https://spring.io/blog/2019/12/05/a-bootiful-podcast-pivotal-s-katrina-bakas-about-the-pivotal-healthwatch-product-kubernetes-cloud-foundry-and-so-much-more" />
    <category term="engineering" label="Engineering" />
    <author>
      <name>Josh Long</name>
    </author>
    <id>tag:spring.io,2019-12-05:3900</id>
    <updated>2019-12-05T23:32:08Z</updated>
    <content type="html">&lt;p&gt;Hi, Spring fans! This week Josh Long (&lt;a href="http://twitter.com/starbuxman"&gt;@starbuxman&lt;/a&gt;) talks to Pivotal&amp;rsquo;s Katrina Bakas about the Pivotal HealthWatch product, Kubernetes, Cloud Foundry and so much more.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href="https://medium.com/@kvbakas"&gt;Katrina&amp;rsquo;s Medium blog&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;iframe width="100%" height="300" scrolling="no" frameborder="no" allow="autoplay" src="https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/723382387&amp;color=%23ff5500&amp;auto_play=false&amp;hide_related=false&amp;show_comments=true&amp;show_user=true&amp;show_reposts=false&amp;show_teaser=true&amp;visual=true"&gt;&lt;/iframe&gt;
&lt;!-- rendered by Sagan Renderer Service --&gt;</content>
  </entry>
  <entry>
    <title>Stream Processing with Spring Cloud Stream and Apache Kafka Streams. Part 4 - Error Handling</title>
    <link rel="alternate" href="https://spring.io/blog/2019/12/05/stream-processing-with-spring-cloud-stream-and-apache-kafka-streams-part-4-error-handling" />
    <category term="engineering" label="Engineering" />
    <author>
      <name>Soby Chacko</name>
    </author>
    <id>tag:spring.io,2019-12-05:3897</id>
    <updated>2019-12-05T15:44:35Z</updated>
    <content type="html">&lt;p&gt;Part 1 - &lt;a href="https://spring.io/blog/2019/12/02/stream-processing-with-spring-cloud-stream-and-apache-kafka-streams-part-1-programming-model"&gt;Programming Model&lt;/a&gt;&lt;br/&gt;Part 2 - &lt;a href="https://spring.io/blog/2019/12/03/stream-processing-with-spring-cloud-stream-and-apache-kafka-streams-part-2-programming-model-continued"&gt;Programming Model Continued&lt;/a&gt;&lt;br/&gt;Part 3 - &lt;a href="https://spring.io/blog/2019/12/04/stream-processing-with-spring-cloud-stream-and-apache-kafka-streams-part-3-data-deserialization-and-serialization"&gt;Data deserialization and serialization&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Continuing with the series on looking at the Spring Cloud Stream binder for Kafka Streams, in this blog post, we are looking at the various error-handling strategies that are available in the Kafka Streams binder.&lt;/p&gt;
&lt;p&gt;The error handling in Kafka Streams is largely centered around errors that occur during deserialization on the inbound and during production on the outbound. &lt;/p&gt;&lt;h2&gt;&lt;a href="#handling-deserialization-exceptions" class="anchor" name="handling-deserialization-exceptions"&gt;&lt;/a&gt;Handling Deserialization Exceptions&lt;/h2&gt;
&lt;p&gt;Kafka Streams lets you register deserialization exception handlers. The default behavior is that, when you have a deserialization exception, it logs that error and fails the application (&lt;code&gt;LogAndFailExceptionHandler&lt;/code&gt;). It also lets you log and skip the record and continue the application (&lt;code&gt;LogAndContinueExceptionHandler&lt;/code&gt;). Normally, you provide the corresponding classes as part of the configuration. By using the binder, you can set these exception handlers either at the binder level, which will be applicable for the entire application or at the binding level, which gives you more fine-grained control.&lt;/p&gt;
&lt;p&gt;Here¡¯s how you can set the deserialization exception handlers at the binder level:&lt;/p&gt;
&lt;pre&gt;&lt;code class="prettyprint"&gt;spring.cloud.stream.kafka.streams.binder.deserializationExceptionHandler=logAndContinue
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you have only a single processor with a single input, it is an easy way to set the deserialization exception handler on the binder as shown above. If you have multiple processors or inputs and if you want to control error handling on them separately, that needs to be set per input binding. Here is an example of doing so:&lt;/p&gt;
&lt;pre&gt;&lt;code class="prettyprint"&gt;spring.cloud.stream.kafka.streams.bindings.process-in-0.consumer.deserializationExceptionHandler=logAndContinue
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice that the handler is actually set on the input binding &lt;code&gt;process-in-0&lt;/code&gt;. If you have more such input bindings, then that has to be explicitly set.&lt;/p&gt;&lt;h2&gt;&lt;a href="#kafka-streams-and-the-dlq-dead-letter-queue" class="anchor" name="kafka-streams-and-the-dlq-dead-letter-queue"&gt;&lt;/a&gt;Kafka Streams and the DLQ (Dead Letter Queue)&lt;/h2&gt;
&lt;p&gt;In addition to the two exception handlers that Kafka Streams provides, the binder provides a third option: a custom handler that lets you send the record in a deserialization error to a special DLQ. In order to activate this, you have to opt-in for this either at the binder or binding level, as explained above. &lt;/p&gt;
&lt;p&gt;Here¡¯s how to do so:&lt;/p&gt;
&lt;pre&gt;&lt;code class="prettyprint"&gt;spring.cloud.stream.kafka.streams.binder.deserializationExceptionHandler=sendToDlq
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Keep in mind that, when using this setting at the binder, this activates the DLQ at the global level, and this will be applied against all the input topics through their bindings. If that&amp;rsquo;s not what you want to happen, you have to enable it per input binding. &lt;/p&gt;
&lt;p&gt;By default, the DLQ name is named &lt;code&gt;error.&amp;lt;input-topic-name&amp;gt;.&amp;lt;application-id for kafka streams&amp;gt;&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;You can replace &lt;code&gt;&amp;lt;input-topic-name&amp;gt;&lt;/code&gt; with the actual topic name. Note that this is &lt;strong&gt;not&lt;/strong&gt; the binding name but the actual topic name. &lt;/p&gt;
&lt;p&gt;If the input topic is topic-1 and the Kafka Streams application ID is my-application, the default DLQ name will be &lt;code&gt;error.topic-1.my-application&lt;/code&gt;.&lt;/p&gt;&lt;h2&gt;&lt;a href="#changing-the-default-dlq-name-generated-by-the-binder" class="anchor" name="changing-the-default-dlq-name-generated-by-the-binder"&gt;&lt;/a&gt;Changing the default DLQ name generated by the binder:&lt;/h2&gt;
&lt;p&gt;You can reset the default DLQ name, as follows:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;spring.cloud.stream.bindings.process-in-0.consumer.dlqName=input-1-dlq&lt;/code&gt; (Replace &lt;code&gt;process-in-0&lt;/code&gt; with the actual binding name)&lt;/p&gt;
&lt;p&gt;If it has the required permissions on the broker, the binder provisioner will create all the necessary DLQ topics. If that&amp;rsquo;s not the case, these topics have to be created manually before the application starts.&lt;/p&gt;&lt;h2&gt;&lt;a href="#dlq-topic-and-partitions" class="anchor" name="dlq-topic-and-partitions"&gt;&lt;/a&gt;DLQ Topic and Partitions&lt;/h2&gt;
&lt;p&gt;By default, the binder assumes that the DLQ topic is provisioned with the same number of partitions as the input topic. If that¡¯s not true (that is if the DLQ topic is provisioned with a different number of partitions), you have to tell the binder the partition to which to send the records by using a &lt;a href="https://github.com/spring-cloud/spring-cloud-stream-binder-kafka/blob/master/spring-cloud-stream-binder-kafka-core/src/main/java/org/springframework/cloud/stream/binder/kafka/utils/DlqPartitionFunction.java"&gt;DlqPartitionFunction&lt;/a&gt; implementation, as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code class="prettyprint"&gt;@Bean&#xD;
public DlqPartitionFunction partitionFunction() {&#xD;
    return (group, record, ex) -&amp;gt; 0;&#xD;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There can only be one such bean present in the application. Therefore, you have to filter out the records by using a group (which is the same as the application ID when using the binder) in the event of multiple processors or inputs with separate DLQ topics.&lt;/p&gt;&lt;h2&gt;&lt;a href="#handling-producer-errors" class="anchor" name="handling-producer-errors"&gt;&lt;/a&gt;Handling producer errors&lt;/h2&gt;
&lt;p&gt;All the exception handlers that we discussed so far deal only with errors surrounding deserialization of data. Kafka Streams also provides an ability to handle producer errors on the outbound. As of the 3.0. Release, the binder does not provide a first-class mechanism to support this. However, this doesn¡¯t mean that you can¡¯t use the producer exception handlers. You can use the various customizers that the binder relies on from &lt;a href="https://spring.io/projects/spring-kafka"&gt;Spring for Apache Kafka project&lt;/a&gt; to do that. These customizers are going to be the topic of our next blog post in this series.&lt;/p&gt;&lt;h2&gt;&lt;a href="#kafka-streams-binder-health-indicator-and-metrics" class="anchor" name="kafka-streams-binder-health-indicator-and-metrics"&gt;&lt;/a&gt;Kafka Streams Binder Health Indicator and Metrics&lt;/h2&gt;
&lt;p&gt;Kafka Streams binder allows the monitoring of the health of the underlying streams thread and it exposes the health-indicator metrics through a Spring Boot actuator endpoint. You can find more details &lt;a href="https://cloud.spring.io/spring-cloud-static/spring-cloud-stream-binder-kafka/3.0.0.RELEASE/reference/html/spring-cloud-stream-binder-kafka.html#_health_indicator"&gt;here&lt;/a&gt;. In addition to the health indicator, the binder also exposes Kafka Streams metrics through Micrometer meter-registry. All the basic metrics available through the KafkaStreams object is available in this registry. &lt;a href="https://cloud.spring.io/spring-cloud-static/spring-cloud-stream-binder-kafka/3.0.0.RELEASE/reference/html/spring-cloud-stream-binder-kafka.html#_accessing_kafka_streams_metrics"&gt;Here&lt;/a&gt; is where you can find more information on this. &lt;/p&gt;&lt;h2&gt;&lt;a href="#summary" class="anchor" name="summary"&gt;&lt;/a&gt;Summary&lt;/h2&gt;
&lt;p&gt;In this blog post, we saw the various strategies Kafka Streams uses to enable handling deserialization exceptions. On top of these, the Kafka Streams binder also provides a handler that lets you send error-prone payloads to a DLQ topic. We saw that the binder provides fine-grained control of working with these DLQ topics. &lt;/p&gt;
&lt;p&gt;Thank you for reading this far! In the next blog post, we are going to see how the binder enables further customizations.&lt;/p&gt;
&lt;!-- rendered by Sagan Renderer Service --&gt;</content>
  </entry>
  <entry>
    <title>Spring Batch 4.0.4, 4.1.3 and 4.2.1 available now!</title>
    <link rel="alternate" href="https://spring.io/blog/2019/12/04/spring-batch-4-0-4-4-1-3-and-4-2-1-available-now" />
    <category term="releases" label="Releases" />
    <author>
      <name>Mahmoud Ben Hassine</name>
    </author>
    <id>tag:spring.io,2019-12-05:3898</id>
    <updated>2019-12-04T22:27:00Z</updated>
    <content type="html">&lt;p&gt;I am pleased to announce the release of Spring Batch 4.0.4, 4.1.3 and 4.2.1 with bug fixes as well as documentation and dependencies updates. Please find the complete list of changes in the release notes: &lt;a href="https://jira.spring.io/secure/ReleaseNote.jspa?projectId=10090&amp;version=17484"&gt;4.0.4&lt;/a&gt;, &lt;a href="https://jira.spring.io/secure/ReleaseNote.jspa?projectId=10090&amp;version=17485"&gt;4.1.3&lt;/a&gt;, &lt;a href="https://jira.spring.io/secure/ReleaseNote.jspa?projectId=10090&amp;version=17741"&gt;4.2.1&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;As we &lt;a href="https://spring.io/blog/2019/10/02/spring-batch-4-2-in-now-ga#what-rsquo-s-next"&gt;announced&lt;/a&gt; earlier this year, version 4.0.4 is the last release of the 4.0 line. The 4.1.x line will get another bug fix release next year and 4.1.4 will be the last release for this line. Please upgrade to v4.2+ at your earliest convenience as this is the primary active branch for the moment and which will be supported until the end of 2020.&lt;/p&gt;
&lt;p&gt;The next feature release will be 4.3, with a GA planned for October 2020, aligned with Spring Framework 5.3 and Spring Boot 2.4. This release is expected to be the last feature branch of Spring Batch 4.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://spring.io/projects/spring-batch"&gt;Spring Batch Home&lt;/a&gt; | &lt;a href="https://github.com/spring-projects/spring-batch"&gt;Source on GitHub&lt;/a&gt; | &lt;a href="https://docs.spring.io/spring-batch/4.2.x/reference/html/index.html"&gt;Reference Documentation&lt;/a&gt;&lt;/p&gt;
&lt;!-- rendered by Sagan Renderer Service --&gt;</content>
  </entry>
  <entry>
    <title>Stream Processing with Spring Cloud Stream and Apache Kafka Streams. Part 3 - Data deserialization and serialization</title>
    <link rel="alternate" href="https://spring.io/blog/2019/12/04/stream-processing-with-spring-cloud-stream-and-apache-kafka-streams-part-3-data-deserialization-and-serialization" />
    <category term="engineering" label="Engineering" />
    <author>
      <name>Soby Chacko</name>
    </author>
    <id>tag:spring.io,2019-12-04:3896</id>
    <updated>2019-12-04T16:45:21Z</updated>
    <content type="html">&lt;p&gt;Part 1 - &lt;a href="https://spring.io/blog/2019/12/02/stream-processing-with-spring-cloud-stream-and-apache-kafka-streams-part-1-programming-model"&gt;Programming Model&lt;/a&gt;&lt;br/&gt;Part 2 - &lt;a href="https://spring.io/blog/2019/12/03/stream-processing-with-spring-cloud-stream-and-apache-kafka-streams-part-2-programming-model-continued"&gt;Programming Model Continued&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Continuing on the previous two blog posts, in this series on writing stream processing applications with Spring Cloud Stream and Kafka Streams, now we will look at the details of how these applications handle deserialization on the inbound and serialization on the outbound. &lt;/p&gt;
&lt;p&gt;All three major higher-level types in Kafka Streams - &lt;code&gt;KStream&amp;lt;K,V&amp;gt;&lt;/code&gt;, &lt;code&gt;KTable&amp;lt;K,V&amp;gt;&lt;/code&gt; and &lt;code&gt;GlobalKTable&amp;lt;K,V&amp;gt;&lt;/code&gt; - work with a key and a value. &lt;/p&gt;
&lt;p&gt;With Spring Cloud Stream Kafka Streams support, keys are always deserialized and serialized by using the native &lt;code&gt;Serde&lt;/code&gt; mechanism. A &lt;code&gt;Serde&lt;/code&gt; is a container object where it provides a deserializer and a serializer. &lt;/p&gt;
&lt;p&gt;Values, on the other hand, are marshaled by using either &lt;code&gt;Serde&lt;/code&gt; or the binder-provided message conversion. Starting with version 3.0 of the binder, using &lt;code&gt;Serde&lt;/code&gt; is the default approach. Using the message converters in Spring is an optional feature that you only need to use on special occasions.&lt;/p&gt;
&lt;p&gt;Let¡¯s look at this processor:&lt;/p&gt;
&lt;pre&gt;&lt;code class="prettyprint"&gt;@Bean&#xD;
public BiFunction&amp;lt;KStream&amp;lt;String, Long&amp;gt;, KTable&amp;lt;String, String&amp;gt;, KStream&amp;lt;String, Long&amp;gt;&amp;gt; process() {&#xD;
  return (userClicksStream, userRegionsTable) -&amp;gt; (userClicksStream&#xD;
        .leftJoin(userRegionsTable, (clicks, region) -&amp;gt; new RegionWithClicks(region == null ?&#xD;
                    &amp;quot;UNKNOWN&amp;quot; : region, clicks),&#xD;
              Joined.with(Serdes.String(), Serdes.Long(), null))&#xD;
        .map((user, regionWithClicks) -&amp;gt; new KeyValue&amp;lt;&amp;gt;(regionWithClicks.getRegion(),&#xD;
              regionWithClicks.getClicks()))&#xD;
        .groupByKey(Grouped.with(Serdes.String(), Serdes.Long()))&#xD;
        .reduce(Long::sum)&#xD;
        .toStream());&#xD;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is the same processor we saw in the &lt;a href="https://spring.io/blog/2019/12/03/stream-processing-with-spring-cloud-stream-and-apache-kafka-streams-part-2-programming-model-continued"&gt;previous blog&lt;/a&gt;. It has two inputs and an output. The first input binding is a &lt;code&gt;KStream&amp;lt;String, Long&amp;gt;&lt;/code&gt;. The key is of type &lt;code&gt;String&lt;/code&gt; and the value is a &lt;code&gt;Long&lt;/code&gt;. The next input binding is a &lt;code&gt;KTable&amp;lt;String, String&amp;gt;&lt;/code&gt;. Here, both key and value are of type &lt;code&gt;String.&lt;/code&gt; Finally, the output binding is a &lt;code&gt;KStream&amp;lt;String, Long&amp;gt;&lt;/code&gt; with the key as a &lt;code&gt;String&lt;/code&gt; and the value as a &lt;code&gt;Long&lt;/code&gt;. &lt;/p&gt;
&lt;p&gt;Normally, you have to tell the application the right &lt;code&gt;Serde&lt;/code&gt; to use as part of the application¡¯s configuration. However, when using the Kafka Streams binder, for most standard types, this information is inferred and you don¡¯t need to provide any special configuration.&lt;/p&gt;
&lt;p&gt;The types that are inferred by the binder are those for which Kafka Streams provides out of the box &lt;code&gt;Serde&lt;/code&gt; implementations. These are those types:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Integer&lt;/li&gt;
  &lt;li&gt;Long&lt;/li&gt;
  &lt;li&gt;Short&lt;/li&gt;
  &lt;li&gt;Double&lt;/li&gt;
  &lt;li&gt;Float&lt;/li&gt;
  &lt;li&gt;Byte[]&lt;/li&gt;
  &lt;li&gt;UUID&lt;/li&gt;
  &lt;li&gt;String&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In other words, if your &lt;code&gt;KStream&lt;/code&gt;, &lt;code&gt;KTable&lt;/code&gt;, or &lt;code&gt;GlobalKTable&lt;/code&gt; have these as the types for the key and the value, you don¡¯t need to provide any special &lt;code&gt;Serde&lt;/code&gt; configuration.&lt;/p&gt;&lt;h2&gt;&lt;a href="#providing-serde-objects-as-spring-beans" class="anchor" name="providing-serde-objects-as-spring-beans"&gt;&lt;/a&gt;Providing Serde objects as Spring Beans&lt;/h2&gt;
&lt;p&gt;If the types are not from one of these, you can provide a bean of type &lt;code&gt;Serde&amp;lt;T&amp;gt;&lt;/code&gt;, and, if the generic type &lt;code&gt;T&lt;/code&gt; matches with the actual type, the binder will delegate that as the &lt;code&gt;Serde&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;For example, let&amp;rsquo;s say you have the following function signature:&lt;/p&gt;
&lt;pre&gt;&lt;code class="prettyprint"&gt;@Bean&#xD;
publicFunction&amp;lt;KStream&amp;lt;CustomKey, AvroIn&amp;gt;, KStream&amp;lt;CustomKey, AvroOut&amp;gt;&amp;gt; process() {&#xD;
&#xD;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, the key and value types don¡¯t match with any of the known &lt;code&gt;Serde&lt;/code&gt; implementations. In that case, you have two options. The recommended approach is to provide a &lt;code&gt;Serde&lt;/code&gt; bean, as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code class="prettyprint"&gt;@Bean&#xD;
public Serde&amp;lt;CustomKey&amp;gt; customKeySerde(){ &#xD;
  	return new CustomKeySerde();&#xD;
}&#xD;
&#xD;
@Bean&#xD;
public Serde&amp;lt;AvroIn&amp;gt; avroInSerde(){ &#xD;
  	final SpecificAvroSerde&amp;lt;AvroIn&amp;gt; avroInSerde = new SpecificAvroSerde&amp;lt;&amp;gt;();&#xD;
avroInSerde.configure(...);&#xD;
return avroInSerde;&#xD;
&#xD;
}&#xD;
&#xD;
@Bean&#xD;
public Serde&amp;lt;AvroOut&amp;gt; avroInSerde(){ &#xD;
 	final SpecificAvroSerde&amp;lt;AvroOut&amp;gt; avroOutSerde = new SpecificAvroSerde&amp;lt;&amp;gt;();&#xD;
avroOutSerde.configure(...);&#xD;
return avroOutSerde;&#xD;
}
&lt;/code&gt;&lt;/pre&gt;&lt;h2&gt;&lt;a href="#provide-serde-through-configuration" class="anchor" name="provide-serde-through-configuration"&gt;&lt;/a&gt;Provide Serde through Configuration&lt;/h2&gt;
&lt;p&gt;If you don¡¯t want to provide &lt;code&gt;Serde&lt;/code&gt; as programmatically created Spring beans, you can also define these by using configuration, where you pass the fully qualified name of the &lt;code&gt;Serde&lt;/code&gt; implementation class, as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code class="prettyprint"&gt;spring.cloud.stream.kafka.streams.bindings.process-in-0.consumer.keySerde=CustomKeySerde&#xD;
spring.cloud.stream.kafka.streams.bindings.process-in-0.consumer.valueSerde=io.confluent.kafka.streams.serdes.avro.SpecificAvroSerde&#xD;
&#xD;
spring.cloud.stream.kafka.streams.bindings.process-out-0.producer.keySerde=CustomKeySerde&#xD;
spring.cloud.stream.kafka.streams.bindings.process-out-0.producer.valueSerde=io.confluent.kafka.streams.serdes.avro.SpecificAvroSerde
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;By the way, setting Serde like this will have higher precedence even if you have matching beans since these configurations are set on the actual consumer and producer bindings. The binder gives it precedence since the user explicitly requested it. &lt;/p&gt;&lt;h2&gt;&lt;a href="#default-serde-and-falling-back-to-jsonserde" class="anchor" name="default-serde-and-falling-back-to-jsonserde"&gt;&lt;/a&gt;Default Serde and falling back to JsonSerde&lt;/h2&gt;
&lt;p&gt;At this point, if the binder still cannot match any &lt;code&gt;Serde&lt;/code&gt;, it looks for a default one to match.&lt;/p&gt;
&lt;p&gt;If all approaches fail to match one, the binder will fall back to the &lt;a href="https://docs.spring.io/spring-kafka/api/org/springframework/kafka/support/serializer/JsonSerde.html"&gt;JsonSerde&lt;/a&gt; implementation provided by Spring for Apache Kafka project. If you don¡¯t use any of the above mechanisms and let the binder fall back to &lt;code&gt;JsonSerde&lt;/code&gt;, you have to make sure that the classes are JSON-friendly.&lt;/p&gt;&lt;h2&gt;&lt;a href="#serde-used-inside-the-actual-business-logic" class="anchor" name="serde-used-inside-the-actual-business-logic"&gt;&lt;/a&gt;Serde used inside the actual business logic&lt;/h2&gt;
&lt;p&gt;Kafka Streams has several API methods that need access to &lt;code&gt;Serde&lt;/code&gt; objects. For example, look at the method calls &lt;code&gt;joined&lt;/code&gt; or &lt;code&gt;groupBy&lt;/code&gt; from the earlier &lt;code&gt;BiFunction&lt;/code&gt; example processor. This is actually the responsibility of the application developer to provide, as the binder cannot help with any inference in those instances. In other words, the binder support for &lt;code&gt;Serde&lt;/code&gt; inference, matching a &lt;code&gt;Serde&lt;/code&gt; with a provided bean, and so on are applied only on the edges of your application, at either the input or the output bindings. Confusion may arise because, when you use the binder for developing Kafka Streams applications, you might think that the binder will completely hide the complexities of &lt;code&gt;Serde&lt;/code&gt;, which is a false impression. The binder helps you with the &lt;code&gt;Serde&lt;/code&gt; only on consuming and producing. Any &lt;code&gt;Serde&lt;/code&gt; required by your business logic implementation still needs to be provided by the application.&lt;/p&gt;&lt;h2&gt;&lt;a href="#summary" class="anchor" name="summary"&gt;&lt;/a&gt;Summary&lt;/h2&gt;
&lt;p&gt;In this blog post, we saw an overview of how the Kafka Streams binder for Spring Cloud Stream helps you with deserialization and serialization of the data. The binder can infer the key and value types used on the input and output bindings. We saw that the default is to always use native &lt;code&gt;Serde&lt;/code&gt; mechanism, but the binder gives you an option to disable this and delegate to Spring¡¯s message converters if need be. We also found out that any &lt;code&gt;Serde&lt;/code&gt; required by your business logic implementation still needs to be provided by the application. &lt;/p&gt;
&lt;p&gt;In the next blog post, we will look at the various error handling mechanisms that Kafka Streams provides for deserialization and production of messages and how the binder supports them. &lt;/p&gt;
&lt;!-- rendered by Sagan Renderer Service --&gt;</content>
  </entry>
  <entry>
    <title>Spring Data Moore SR3 and Lovelace SR14 released</title>
    <link rel="alternate" href="https://spring.io/blog/2019/12/04/spring-data-moore-sr3-and-lovelace-sr14-released" />
    <category term="releases" label="Releases" />
    <author>
      <name>Jens Schauder</name>
    </author>
    <id>tag:spring.io,2019-12-04:3895</id>
    <updated>2019-12-04T14:58:31Z</updated>
    <content type="html">&lt;p&gt;On behalf of the community, we are pleased to announce that Spring Data &lt;code&gt;Moore SR3&lt;/code&gt; and &lt;code&gt;Lovelace SR14&lt;/code&gt; are now available from Maven Central. Both releases ship with almost 70 tickets in total, mostly bugfixes and dependency upgrades.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Moore SR3&lt;/code&gt; is built on top of the recently released Spring Framework &lt;code&gt;5.2.2&lt;/code&gt; and will be picked up by Spring Boot &lt;code&gt;2.2.2&lt;/code&gt; for easier consumption and &lt;code&gt;Lovelace SR14&lt;/code&gt; is built on top of the recently released Spring Framework &lt;code&gt;5.1.12&lt;/code&gt; and will be picked up by Spring Boot &lt;code&gt;2.1.11&lt;/code&gt; for easier consumption.&lt;/p&gt;
&lt;p&gt;Here are links to the reference documentation, changelogs, and artifacts of the individual project releases:&lt;/p&gt;&lt;h3&gt;&lt;a href="#moore-sr-3" class="anchor" name="moore-sr-3"&gt;&lt;/a&gt;Moore SR 3&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Spring Data Commons 2.2.3 - &lt;a href="https://repo.spring.io/libs-release/org/springframework/data/spring-data-commons/2.2.3.RELEASE"&gt;Artifacts&lt;/a&gt; - &lt;a href="https://docs.spring.io/spring-data/commons/docs/2.2.3.RELEASE/api"&gt;Javadoc&lt;/a&gt; - &lt;a href="https://docs.spring.io/spring-data/commons/docs/2.2.3.RELEASE/reference/html"&gt;Documentation&lt;/a&gt; - &lt;a href="https://docs.spring.io/spring-data/commons/docs/2.2.3.RELEASE/changelog.txt"&gt;Changelog&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Spring Data JDBC 1.1.3 - &lt;a href="https://repo.spring.io/libs-release/org/springframework/data/spring-data-jdbc/1.1.3.RELEASE"&gt;Artifacts&lt;/a&gt; - &lt;a href="https://docs.spring.io/spring-data/jdbc/docs/1.1.3.RELEASE/api"&gt;Javadoc&lt;/a&gt; - &lt;a href="https://docs.spring.io/spring-data/jdbc/docs/1.1.3.RELEASE/reference/html"&gt;Documentation&lt;/a&gt; - &lt;a href="https://docs.spring.io/spring-data/jdbc/docs/1.1.3.RELEASE/changelog.txt"&gt;Changelog&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Spring Data JPA 2.2.3 - &lt;a href="https://repo.spring.io/libs-release/org/springframework/data/spring-data-jpa/2.2.3.RELEASE"&gt;Artifacts&lt;/a&gt; - &lt;a href="https://docs.spring.io/spring-data/jpa/docs/2.2.3.RELEASE/api"&gt;Javadoc&lt;/a&gt; - &lt;a href="https://docs.spring.io/spring-data/jpa/docs/2.2.3.RELEASE/reference/html"&gt;Documentation&lt;/a&gt; - &lt;a href="https://docs.spring.io/spring-data/jpa/docs/2.2.3.RELEASE/changelog.txt"&gt;Changelog&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Spring Data for Apache Solr 4.1.3 - &lt;a href="https://repo.spring.io/libs-release/org/springframework/data/spring-data-solr/4.1.3.RELEASE"&gt;Artifacts&lt;/a&gt; - &lt;a href="https://docs.spring.io/spring-data/solr/docs/4.1.3.RELEASE/api"&gt;Javadoc&lt;/a&gt; - &lt;a href="https://docs.spring.io/spring-data/solr/docs/4.1.3.RELEASE/reference/html"&gt;Documentation&lt;/a&gt; - &lt;a href="https://docs.spring.io/spring-data/solr/docs/4.1.3.RELEASE/changelog.txt"&gt;Changelog&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Spring Data Neo4j 5.2.3 - &lt;a href="https://repo.spring.io/libs-release/org/springframework/data/spring-data-neo4j/5.2.3.RELEASE"&gt;Artifacts&lt;/a&gt; - &lt;a href="https://docs.spring.io/spring-data/neo4j/docs/5.2.3.RELEASE/api"&gt;Javadoc&lt;/a&gt; - &lt;a href="https://docs.spring.io/spring-data/neo4j/docs/5.2.3.RELEASE/reference/html"&gt;Documentation&lt;/a&gt; - &lt;a href="https://docs.spring.io/spring-data/neo4j/docs/5.2.3.RELEASE/changelog.txt"&gt;Changelog&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Spring Data for Apache Cassandra 2.2.3 - &lt;a href="https://repo.spring.io/libs-release/org/springframework/data/spring-data-cassandra/2.2.3.RELEASE"&gt;Artifacts&lt;/a&gt; - &lt;a href="https://docs.spring.io/spring-data/cassandra/docs/2.2.3.RELEASE/api"&gt;Javadoc&lt;/a&gt; - &lt;a href="https://docs.spring.io/spring-data/cassandra/docs/2.2.3.RELEASE/reference/html"&gt;Documentation&lt;/a&gt; - &lt;a href="https://docs.spring.io/spring-data/cassandra/docs/2.2.3.RELEASE/changelog.txt"&gt;Changelog&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Spring Data MongoDB 2.2.3 - &lt;a href="https://repo.spring.io/libs-release/org/springframework/data/spring-data-mongodb/2.2.3.RELEASE"&gt;Artifacts&lt;/a&gt; - &lt;a href="https://docs.spring.io/spring-data/mongodb/docs/2.2.3.RELEASE/api"&gt;Javadoc&lt;/a&gt; - &lt;a href="https://docs.spring.io/spring-data/mongodb/docs/2.2.3.RELEASE/reference/html"&gt;Documentation&lt;/a&gt; - &lt;a href="https://docs.spring.io/spring-data/mongodb/docs/2.2.3.RELEASE/changelog.txt"&gt;Changelog&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Spring Data KeyValue 2.2.3 - &lt;a href="https://repo.spring.io/libs-release/org/springframework/data/spring-data-keyvalue/2.2.3.RELEASE"&gt;Artifacts&lt;/a&gt; - &lt;a href="https://docs.spring.io/spring-data/keyvalue/docs/2.2.3.RELEASE/api"&gt;Javadoc&lt;/a&gt; - &lt;a href="https://docs.spring.io/spring-data/keyvalue/docs/2.2.3.RELEASE/reference/html"&gt;Documentation&lt;/a&gt; - &lt;a href="https://docs.spring.io/spring-data/keyvalue/docs/2.2.3.RELEASE/changelog.txt"&gt;Changelog&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Spring Data Gemfire 2.2.3 - &lt;a href="https://repo.spring.io/libs-release/org/springframework/data/spring-data-gemfire/2.2.3.RELEASE"&gt;Artifacts&lt;/a&gt; - &lt;a href="https://docs.spring.io/spring-data/gemfire/docs/2.2.3.RELEASE/api"&gt;Javadoc&lt;/a&gt; - &lt;a href="https://docs.spring.io/spring-data/gemfire/docs/2.2.3.RELEASE/reference/html"&gt;Documentation&lt;/a&gt; - &lt;a href="https://docs.spring.io/spring-data/gemfire/docs/2.2.3.RELEASE/changelog.txt"&gt;Changelog&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Spring Data for Apache Geode 2.2.3 - &lt;a href="https://repo.spring.io/libs-release/org/springframework/data/spring-data-geode/2.2.3.RELEASE"&gt;Artifacts&lt;/a&gt; - &lt;a href="https://docs.spring.io/spring-data/geode/docs/2.2.3.RELEASE/api"&gt;Javadoc&lt;/a&gt; - &lt;a href="https://docs.spring.io/spring-data/geode/docs/2.2.3.RELEASE/reference/html"&gt;Documentation&lt;/a&gt; - &lt;a href="https://docs.spring.io/spring-data/geode/docs/2.2.3.RELEASE/changelog.txt"&gt;Changelog&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Spring Data LDAP 2.2.3 - &lt;a href="https://repo.spring.io/libs-release/org/springframework/data/spring-data-ldap/2.2.3.RELEASE"&gt;Artifacts&lt;/a&gt; - &lt;a href="https://docs.spring.io/spring-data/ldap/docs/2.2.3.RELEASE/api"&gt;Javadoc&lt;/a&gt; - &lt;a href="https://docs.spring.io/spring-data/ldap/docs/2.2.3.RELEASE/reference/html"&gt;Documentation&lt;/a&gt; - &lt;a href="https://docs.spring.io/spring-data/ldap/docs/2.2.3.RELEASE/changelog.txt"&gt;Changelog&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Spring Data Envers 2.2.3 - &lt;a href="https://repo.spring.io/libs-release/org/springframework/data/spring-data-envers/2.2.3.RELEASE"&gt;Artifacts&lt;/a&gt; - &lt;a href="https://docs.spring.io/spring-data/envers/docs/2.2.3.RELEASE/api"&gt;Javadoc&lt;/a&gt; - &lt;a href="https://docs.spring.io/spring-data/envers/docs/2.2.3.RELEASE/reference/html"&gt;Documentation&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Spring Data REST 3.2.3 - &lt;a href="https://repo.spring.io/libs-release/org/springframework/data/spring-data-rest-webmvc/3.2.3.RELEASE"&gt;Artifacts&lt;/a&gt; - &lt;a href="https://docs.spring.io/spring-data/rest/docs/3.2.3.RELEASE/api"&gt;Javadoc&lt;/a&gt; - &lt;a href="https://docs.spring.io/spring-data/rest/docs/3.2.3.RELEASE/reference/html"&gt;Documentation&lt;/a&gt; - &lt;a href="https://docs.spring.io/spring-data/rest/docs/3.2.3.RELEASE/changelog.txt"&gt;Changelog&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Spring Data Redis 2.2.3 - &lt;a href="https://repo.spring.io/libs-release/org/springframework/data/spring-data-redis/2.2.3.RELEASE"&gt;Artifacts&lt;/a&gt; - &lt;a href="https://docs.spring.io/spring-data/redis/docs/2.2.3.RELEASE/api"&gt;Javadoc&lt;/a&gt; - &lt;a href="https://docs.spring.io/spring-data/redis/docs/2.2.3.RELEASE/reference/html"&gt;Documentation&lt;/a&gt; - &lt;a href="https://docs.spring.io/spring-data/redis/docs/2.2.3.RELEASE/changelog.txt"&gt;Changelog&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Spring Data Elasticsearch 3.2.3 - &lt;a href="https://repo.spring.io/libs-release/org/springframework/data/spring-data-elasticsearch/3.2.3.RELEASE"&gt;Artifacts&lt;/a&gt; - &lt;a href="https://docs.spring.io/spring-data/elasticsearch/docs/3.2.3.RELEASE/api"&gt;Javadoc&lt;/a&gt; - &lt;a href="https://docs.spring.io/spring-data/elasticsearch/docs/3.2.3.RELEASE/reference/html"&gt;Documentation&lt;/a&gt; - &lt;a href="https://docs.spring.io/spring-data/elasticsearch/docs/3.2.3.RELEASE/changelog.txt"&gt;Changelog&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Spring Data Couchbase 3.2.3 - &lt;a href="https://repo.spring.io/libs-release/org/springframework/data/spring-data-couchbase/3.2.3.RELEASE"&gt;Artifacts&lt;/a&gt; - &lt;a href="https://docs.spring.io/spring-data/couchbase/docs/3.2.3.RELEASE/api"&gt;Javadoc&lt;/a&gt; - &lt;a href="https://docs.spring.io/spring-data/couchbase/docs/3.2.3.RELEASE/reference/html"&gt;Documentation&lt;/a&gt; - &lt;a href="https://docs.spring.io/spring-data/couchbase/docs/3.2.3.RELEASE/changelog.txt"&gt;Changelog&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;h3&gt;&lt;a href="#lovelace-sr14" class="anchor" name="lovelace-sr14"&gt;&lt;/a&gt;Lovelace SR14&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Spring Data Commons 2.1.14 - &lt;a href="https://repo.spring.io/libs-release/org/springframework/data/spring-data-commons/2.1.14.RELEASE"&gt;Artifacts&lt;/a&gt; - &lt;a href="https://docs.spring.io/spring-data/commons/docs/2.1.14.RELEASE/api"&gt;Javadoc&lt;/a&gt; - &lt;a href="https://docs.spring.io/spring-data/commons/docs/2.1.14.RELEASE/reference/html"&gt;Documentation&lt;/a&gt; - &lt;a href="https://docs.spring.io/spring-data/commons/docs/2.1.14.RELEASE/changelog.txt"&gt;Changelog&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Spring Data JDBC 1.0.14 - &lt;a href="https://repo.spring.io/libs-release/org/springframework/data/spring-data-jdbc/1.0.14.RELEASE"&gt;Artifacts&lt;/a&gt; - &lt;a href="https://docs.spring.io/spring-data/jdbc/docs/1.0.14.RELEASE/api"&gt;Javadoc&lt;/a&gt; - &lt;a href="https://docs.spring.io/spring-data/jdbc/docs/1.0.14.RELEASE/reference/html"&gt;Documentation&lt;/a&gt; - &lt;a href="https://docs.spring.io/spring-data/jdbc/docs/1.0.14.RELEASE/changelog.txt"&gt;Changelog&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Spring Data JPA 2.1.14 - &lt;a href="https://repo.spring.io/libs-release/org/springframework/data/spring-data-jpa/2.1.14.RELEASE"&gt;Artifacts&lt;/a&gt; - &lt;a href="https://docs.spring.io/spring-data/jpa/docs/2.1.14.RELEASE/api"&gt;Javadoc&lt;/a&gt; - &lt;a href="https://docs.spring.io/spring-data/jpa/docs/2.1.14.RELEASE/reference/html"&gt;Documentation&lt;/a&gt; - &lt;a href="https://docs.spring.io/spring-data/jpa/docs/2.1.14.RELEASE/changelog.txt"&gt;Changelog&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Spring Data for Apache Solr 4.0.14 - &lt;a href="https://repo.spring.io/libs-release/org/springframework/data/spring-data-solr/4.0.14.RELEASE"&gt;Artifacts&lt;/a&gt; - &lt;a href="https://docs.spring.io/spring-data/solr/docs/4.0.14.RELEASE/api"&gt;Javadoc&lt;/a&gt; - &lt;a href="https://docs.spring.io/spring-data/solr/docs/4.0.14.RELEASE/reference/html"&gt;Documentation&lt;/a&gt; - &lt;a href="https://docs.spring.io/spring-data/solr/docs/4.0.14.RELEASE/changelog.txt"&gt;Changelog&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Spring Data Neo4j 5.1.14 - &lt;a href="https://repo.spring.io/libs-release/org/springframework/data/spring-data-neo4j/5.1.14.RELEASE"&gt;Artifacts&lt;/a&gt; - &lt;a href="https://docs.spring.io/spring-data/neo4j/docs/5.1.14.RELEASE/api"&gt;Javadoc&lt;/a&gt; - &lt;a href="https://docs.spring.io/spring-data/neo4j/docs/5.1.14.RELEASE/reference/html"&gt;Documentation&lt;/a&gt; - &lt;a href="https://docs.spring.io/spring-data/neo4j/docs/5.1.14.RELEASE/changelog.txt"&gt;Changelog&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Spring Data for Apache Cassandra 2.1.14 - &lt;a href="https://repo.spring.io/libs-release/org/springframework/data/spring-data-cassandra/2.1.14.RELEASE"&gt;Artifacts&lt;/a&gt; - &lt;a href="https://docs.spring.io/spring-data/cassandra/docs/2.1.14.RELEASE/api"&gt;Javadoc&lt;/a&gt; - &lt;a href="https://docs.spring.io/spring-data/cassandra/docs/2.1.14.RELEASE/reference/html"&gt;Documentation&lt;/a&gt; - &lt;a href="https://docs.spring.io/spring-data/cassandra/docs/2.1.14.RELEASE/changelog.txt"&gt;Changelog&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Spring Data MongoDB 2.1.14 - &lt;a href="https://repo.spring.io/libs-release/org/springframework/data/spring-data-mongodb/2.1.14.RELEASE"&gt;Artifacts&lt;/a&gt; - &lt;a href="https://docs.spring.io/spring-data/mongodb/docs/2.1.14.RELEASE/api"&gt;Javadoc&lt;/a&gt; - &lt;a href="https://docs.spring.io/spring-data/mongodb/docs/2.1.14.RELEASE/reference/html"&gt;Documentation&lt;/a&gt; - &lt;a href="https://docs.spring.io/spring-data/mongodb/docs/2.1.14.RELEASE/changelog.txt"&gt;Changelog&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Spring Data KeyValue 2.1.14 - &lt;a href="https://repo.spring.io/libs-release/org/springframework/data/spring-data-keyvalue/2.1.14.RELEASE"&gt;Artifacts&lt;/a&gt; - &lt;a href="https://docs.spring.io/spring-data/keyvalue/docs/2.1.14.RELEASE/api"&gt;Javadoc&lt;/a&gt; - &lt;a href="https://docs.spring.io/spring-data/keyvalue/docs/2.1.14.RELEASE/reference/html"&gt;Documentation&lt;/a&gt; - &lt;a href="https://docs.spring.io/spring-data/keyvalue/docs/2.1.14.RELEASE/changelog.txt"&gt;Changelog&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Spring Data Gemfire 2.1.14 - &lt;a href="https://repo.spring.io/libs-release/org/springframework/data/spring-data-gemfire/2.1.14.RELEASE"&gt;Artifacts&lt;/a&gt; - &lt;a href="https://docs.spring.io/spring-data/gemfire/docs/2.1.14.RELEASE/api"&gt;Javadoc&lt;/a&gt; - &lt;a href="https://docs.spring.io/spring-data/gemfire/docs/2.1.14.RELEASE/reference/html"&gt;Documentation&lt;/a&gt; - &lt;a href="https://docs.spring.io/spring-data/gemfire/docs/2.1.14.RELEASE/changelog.txt"&gt;Changelog&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Spring Data for Apache Geode 2.1.14 - &lt;a href="https://repo.spring.io/libs-release/org/springframework/data/spring-data-geode/2.1.14.RELEASE"&gt;Artifacts&lt;/a&gt; - &lt;a href="https://docs.spring.io/spring-data/geode/docs/2.1.14.RELEASE/api"&gt;Javadoc&lt;/a&gt; - &lt;a href="https://docs.spring.io/spring-data/geode/docs/2.1.14.RELEASE/reference/html"&gt;Documentation&lt;/a&gt; - &lt;a href="https://docs.spring.io/spring-data/geode/docs/2.1.14.RELEASE/changelog.txt"&gt;Changelog&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Spring Data LDAP 2.1.14 - &lt;a href="https://repo.spring.io/libs-release/org/springframework/data/spring-data-ldap/2.1.14.RELEASE"&gt;Artifacts&lt;/a&gt; - &lt;a href="https://docs.spring.io/spring-data/ldap/docs/2.1.14.RELEASE/api"&gt;Javadoc&lt;/a&gt; - &lt;a href="https://docs.spring.io/spring-data/ldap/docs/2.1.14.RELEASE/reference/html"&gt;Documentation&lt;/a&gt; - &lt;a href="https://docs.spring.io/spring-data/ldap/docs/2.1.14.RELEASE/changelog.txt"&gt;Changelog&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Spring Data Envers 2.1.14 - &lt;a href="https://repo.spring.io/libs-release/org/springframework/data/spring-data-envers/2.1.14.RELEASE"&gt;Artifacts&lt;/a&gt; - &lt;a href="https://docs.spring.io/spring-data/envers/docs/2.1.14.RELEASE/api"&gt;Javadoc&lt;/a&gt; - &lt;a href="https://docs.spring.io/spring-data/envers/docs/2.1.14.RELEASE/reference/html"&gt;Documentation&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Spring Data REST 3.1.14 - &lt;a href="https://repo.spring.io/libs-release/org/springframework/data/spring-data-rest-webmvc/3.1.14.RELEASE"&gt;Artifacts&lt;/a&gt; - &lt;a href="https://docs.spring.io/spring-data/rest/docs/3.1.14.RELEASE/api"&gt;Javadoc&lt;/a&gt; - &lt;a href="https://docs.spring.io/spring-data/rest/docs/3.1.14.RELEASE/reference/html"&gt;Documentation&lt;/a&gt; - &lt;a href="https://docs.spring.io/spring-data/rest/docs/3.1.14.RELEASE/changelog.txt"&gt;Changelog&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Spring Data Redis 2.1.14 - &lt;a href="https://repo.spring.io/libs-release/org/springframework/data/spring-data-redis/2.1.14.RELEASE"&gt;Artifacts&lt;/a&gt; - &lt;a href="https://docs.spring.io/spring-data/redis/docs/2.1.14.RELEASE/api"&gt;Javadoc&lt;/a&gt; - &lt;a href="https://docs.spring.io/spring-data/redis/docs/2.1.14.RELEASE/reference/html"&gt;Documentation&lt;/a&gt; - &lt;a href="https://docs.spring.io/spring-data/redis/docs/2.1.14.RELEASE/changelog.txt"&gt;Changelog&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Spring Data Elasticsearch 3.1.14 - &lt;a href="https://repo.spring.io/libs-release/org/springframework/data/spring-data-elasticsearch/3.1.14.RELEASE"&gt;Artifacts&lt;/a&gt; - &lt;a href="https://docs.spring.io/spring-data/elasticsearch/docs/3.1.14.RELEASE/api"&gt;Javadoc&lt;/a&gt; - &lt;a href="https://docs.spring.io/spring-data/elasticsearch/docs/3.1.14.RELEASE/reference/html"&gt;Documentation&lt;/a&gt; - &lt;a href="https://docs.spring.io/spring-data/elasticsearch/docs/3.1.14.RELEASE/changelog.txt"&gt;Changelog&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Spring Data Couchbase 3.1.14 - &lt;a href="https://repo.spring.io/libs-release/org/springframework/data/spring-data-couchbase/3.1.14.RELEASE"&gt;Artifacts&lt;/a&gt; - &lt;a href="https://docs.spring.io/spring-data/couchbase/docs/3.1.14.RELEASE/api"&gt;Javadoc&lt;/a&gt; - &lt;a href="https://docs.spring.io/spring-data/couchbase/docs/3.1.14.RELEASE/reference/html"&gt;Documentation&lt;/a&gt; - &lt;a href="https://docs.spring.io/spring-data/couchbase/docs/3.1.14.RELEASE/changelog.txt"&gt;Changelog&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- rendered by Sagan Renderer Service --&gt;</content>
  </entry>
  <entry>
    <title>Stream Processing with Spring Cloud Stream and Apache Kafka Streams. Part 2 - Programming Model Continued</title>
    <link rel="alternate" href="https://spring.io/blog/2019/12/03/stream-processing-with-spring-cloud-stream-and-apache-kafka-streams-part-2-programming-model-continued" />
    <category term="engineering" label="Engineering" />
    <author>
      <name>Soby Chacko</name>
    </author>
    <id>tag:spring.io,2019-12-02:3892</id>
    <updated>2019-12-03T16:09:00Z</updated>
    <content type="html">&lt;p&gt;On the heels of the &lt;a href="https://spring.io/blog/2019/12/02/stream-processing-with-spring-cloud-stream-and-apache-kafka-streams-part-1-programming-model"&gt;previous blog&lt;/a&gt; in which we introduced the basic functional programming model for writing streaming applications with Spring Cloud Stream and Kafka Streams, in this part, we are going to further explore that programming model.&lt;/p&gt;
&lt;p&gt;Let¡¯s look at a few scenarios.&lt;/p&gt;&lt;h2&gt;&lt;a href="#scenario-1-single-input-and-output-binding" class="anchor" name="scenario-1-single-input-and-output-binding"&gt;&lt;/a&gt;Scenario 1: Single input and output binding&lt;/h2&gt;
&lt;p&gt;If your application consumes data from a single input binding and produces data into an output binding, you can use Java¡¯s Function interface to do that. Keep in mind that binding in this sense is not necessarily mapped to a single input Kafka topic, because topics could be multiplexed and attached to a single input binding (with comma-separated multiple topics configured on a single binding - see below for an example). On the outbound case, the binding maps to a single topic here. &lt;/p&gt;
&lt;p&gt;Here is an example processor:&lt;/p&gt;
&lt;p&gt;Note that the actual business logic implementation is given as a lambda expression in this processor.&lt;/p&gt;
&lt;pre&gt;&lt;code class="prettyprint"&gt;@Bean&#xD;
public Function&amp;lt;KStream&amp;lt;Object, String&amp;gt;, KStream&amp;lt;String, WordCount&amp;gt;&amp;gt; wordcount() {&#xD;
&#xD;
  return input -&amp;gt; input&#xD;
        .flatMapValues(value -&amp;gt; Arrays.asList(value.toLowerCase().split(&amp;quot;\\W+&amp;quot;)))&#xD;
        .map((key, value) -&amp;gt; new KeyValue&amp;lt;&amp;gt;(value, value))&#xD;
        .groupByKey(Grouped.with(Serdes.String(), Serdes.String()))&#xD;
        .windowedBy(TimeWindows.of(5000))&#xD;
        .count(Materialized.as(&amp;quot;wordcount-store&amp;quot;))&#xD;
        .toStream()&#xD;
        .map((key, value) -&amp;gt; new KeyValue&amp;lt;&amp;gt;(key.key(), new WordCount(key.key(), value,&#xD;
              new Date(key.window().start()), new Date(key.window().end()))));&#xD;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Look at the return signature of the processor. It is a &lt;code&gt;Function&amp;lt;KStream&amp;lt;Object, String&amp;gt;, KStream&amp;lt;String, WordCount&amp;gt;&amp;gt;&lt;/code&gt;. The processor consumes a &lt;code&gt;KStream&lt;/code&gt; and produces another &lt;code&gt;KStream&lt;/code&gt; Under the hood, the binder uses an incoming Kafka topic to consume data from and then provide that to this input &lt;code&gt;KStream&lt;/code&gt;. Similarly, on the outbound, the binder produces data as a &lt;code&gt;KStream&lt;/code&gt; which will be sent to an outgoing Kafka topic. &lt;/p&gt;
&lt;p&gt;Here is how you may provide input topics to this processor:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;spring.cloud.stream.bindings.wordcount-in-0.destination=words&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;In the case of multiplexed topics, you can use this:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;spring.cloud.stream.bindings.wordcount-in-0.destination=words1,words2,word3&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;The output topic can be configured as below:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;spring.cloud.stream.bindings.wordcount-out-0.destination=counts&lt;/code&gt;&lt;/p&gt;&lt;h2&gt;&lt;a href="#scenario-2-multiple-output-bindings-through-kafka-streams-branching" class="anchor" name="scenario-2-multiple-output-bindings-through-kafka-streams-branching"&gt;&lt;/a&gt;Scenario 2: Multiple output bindings through Kafka Streams branching&lt;/h2&gt;
&lt;p&gt;Kafka Streams lets you send to multiple topics on the outbound by using a feature called branching. Essentially, it uses a predicate to match as a basis for branching into multiple topics. This is largely identical to the example above, but the main difference is that the outbound is provided as a KStream[]. &lt;/p&gt;
&lt;p&gt;Here is an example processor:&lt;/p&gt;
&lt;pre&gt;&lt;code class="prettyprint"&gt;  @Bean&#xD;
  public Function&amp;lt;KStream&amp;lt;Object, String&amp;gt;, KStream&amp;lt;?, WordCount&amp;gt;[]&amp;gt; wordcount() {&#xD;
&#xD;
     Predicate&amp;lt;Object, WordCount&amp;gt; isEnglish = (k, v) -&amp;gt; v.word.equals(&amp;quot;english&amp;quot;);&#xD;
     Predicate&amp;lt;Object, WordCount&amp;gt; isFrench = (k, v) -&amp;gt; v.word.equals(&amp;quot;french&amp;quot;);&#xD;
     Predicate&amp;lt;Object, WordCount&amp;gt; isSpanish = (k, v) -&amp;gt; v.word.equals(&amp;quot;spanish&amp;quot;);&#xD;
&#xD;
     return input -&amp;gt; input&#xD;
           .flatMapValues(value -&amp;gt; Arrays.asList(value.toLowerCase().split(&amp;quot;\\W+&amp;quot;)))&#xD;
           .groupBy((key, value) -&amp;gt; value)&#xD;
           .windowedBy(TimeWindows.of(5000))&#xD;
           .count(Materialized.as(&amp;quot;WordCounts-branch&amp;quot;))&#xD;
           .toStream()&#xD;
           .map((key, value) -&amp;gt; new KeyValue&amp;lt;&amp;gt;(null, new WordCount(key.key(), value,&#xD;
                 new Date(key.window().start()), new Date(key.window().end()))))&#xD;
           .branch(isEnglish, isFrench, isSpanish);&#xD;
  }&#xD;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Pay attention to the second parametric type for the function. It is provided as a KStream[].&lt;/p&gt;
&lt;p&gt;You can provide the individual output topics for these bindings:&lt;/p&gt;
&lt;pre&gt;&lt;code class="prettyprint"&gt;spring.cloud.stream.bindings.wordcount-out-0.destination=output1&#xD;
spring.cloud.stream.bindings.wordcount-out-1.destination=output2&#xD;
spring.cloud.stream.bindings.wordcount-out-2.destination=output3
&lt;/code&gt;&lt;/pre&gt;&lt;h2&gt;&lt;a href="#scenario-3-two-input-bindings-and-a-single-output-binding" class="anchor" name="scenario-3-two-input-bindings-and-a-single-output-binding"&gt;&lt;/a&gt;Scenario 3: Two input bindings and a single output binding.&lt;/h2&gt;
&lt;p&gt;When you have two input bindings and an output binding, you can represent your processor as a bean of type &lt;code&gt;java.util.function.BiFunction&lt;/code&gt;. Here is an example:&lt;/p&gt;
&lt;pre&gt;&lt;code class="prettyprint"&gt;@Bean&#xD;
public BiFunction&amp;lt;KStream&amp;lt;String, Long&amp;gt;, KTable&amp;lt;String, String&amp;gt;, KStream&amp;lt;String, Long&amp;gt;&amp;gt; process() {&#xD;
  return (userClicksStream, userRegionsTable) -&amp;gt; (userClicksStream&#xD;
        .leftJoin(userRegionsTable, (clicks, region) -&amp;gt; new RegionWithClicks(region == null ?&#xD;
                    &amp;quot;UNKNOWN&amp;quot; : region, clicks),&#xD;
              Joined.with(Serdes.String(), Serdes.Long(), null))&#xD;
        .map((user, regionWithClicks) -&amp;gt; new KeyValue&amp;lt;&amp;gt;(regionWithClicks.getRegion(),&#xD;
              regionWithClicks.getClicks()))&#xD;
        .groupByKey(Grouped.with(Serdes.String(), Serdes.Long()))&#xD;
        .reduce(Long::sum)&#xD;
        .toStream());&#xD;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;BiFunction&lt;/code&gt; has two inputs and an output. The first input is a &lt;code&gt;KStream&lt;/code&gt;, and the second one is a &lt;code&gt;KTable&lt;/code&gt;, whereas the output is another &lt;code&gt;KStream&lt;/code&gt;. If you want to have a multiple &lt;code&gt;KStream&lt;/code&gt; on the outbound, you can change the type signature to &lt;code&gt;KStream[]&lt;/code&gt;and then make the necessary implementation changes.&lt;/p&gt;
&lt;p&gt;Scenario 4: Two input bindings and no output bindings&lt;/p&gt;
&lt;p&gt;If you only have two input bindings but no outputs, you can use Java¡¯s &lt;code&gt;BiConsumer&lt;/code&gt; support. Possible use cases are where you don¡¯t want to produce output, but update some state stores. Here is an example:&lt;/p&gt;
&lt;pre&gt;&lt;code class="prettyprint"&gt;@Bean&#xD;
public BiConsumer&amp;lt;KStream&amp;lt;String, Long&amp;gt;, KTable&amp;lt;String, String&amp;gt;&amp;gt; process() {&#xD;
  return (userClicksStream, userRegionsTable) -&amp;gt; {&#xD;
     userClicksStream.foreach((key, value) -&amp;gt; latch.countDown());&#xD;
     userRegionsTable.toStream().foreach((key, value) -&amp;gt; latch.countDown());&#xD;
  };&#xD;
}
&lt;/code&gt;&lt;/pre&gt;&lt;h2&gt;&lt;a href="#scenario-5-more-than-two-input-bindings" class="anchor" name="scenario-5-more-than-two-input-bindings"&gt;&lt;/a&gt;Scenario 5: More than two input bindings.&lt;/h2&gt;
&lt;p&gt;What if you have three or four or n number of input bindings? In that case, you cannot rely on a Function or BiFunction approach. You need to rely on partially applied functions. Basically, you start with a Function, but then, on the outbound of this first function, you provide another Function or Consumer until you exhaust your inputs. This technique of partially applying functions in this way is generally known as function currying in functional programming jargon. Here is an example that uses three inputs and a single output:&lt;/p&gt;
&lt;pre&gt;&lt;code class="prettyprint"&gt;@Bean&#xD;
public Function&amp;lt;KStream&amp;lt;Long, Order&amp;gt;,&#xD;
     Function&amp;lt;GlobalKTable&amp;lt;Long, Customer&amp;gt;,&#xD;
           Function&amp;lt;GlobalKTable&amp;lt;Long, Product&amp;gt;, KStream&amp;lt;Long, EnrichedOrder&amp;gt;&amp;gt;&amp;gt;&amp;gt; process() {&#xD;
&#xD;
  return orderStream -&amp;gt; (&#xD;
        customers -&amp;gt; (&#xD;
              products -&amp;gt; (&#xD;
                    orderStream.join(customers,&#xD;
                          (orderId, order) -&amp;gt; order.getCustomerId(),&#xD;
                          (order, customer) -&amp;gt; new CustomerOrder(customer, order))&#xD;
                          .join(products,&#xD;
                                (orderId, customerOrder) -&amp;gt; customerOrder&#xD;
                                      .productId(),&#xD;
                                (customerOrder, product) -&amp;gt; {&#xD;
                                   EnrichedOrder enrichedOrder = new EnrichedOrder();&#xD;
                                   enrichedOrder.setProduct(product);&#xD;
                                   enrichedOrder.setCustomer(customerOrder.customer);&#xD;
                                   enrichedOrder.setOrder(customerOrder.order);&#xD;
                                   return enrichedOrder;&#xD;
                                })&#xD;
              )&#xD;
        )&#xD;
  );&#xD;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Carefully examine the processor¡¯s type signature.:&lt;/p&gt;
&lt;pre&gt;&lt;code class="prettyprint"&gt;Function&amp;lt;KStream&amp;lt;Long, Order&amp;gt;,&#xD;
     Function&amp;lt;GlobalKTable&amp;lt;Long, Customer&amp;gt;,&#xD;
           Function&amp;lt;GlobalKTable&amp;lt;Long, Product&amp;gt;, KStream&amp;lt;Long, EnrichedOrder&amp;gt;&amp;gt;&amp;gt;&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We start with a function that takes a &lt;code&gt;KStream&lt;/code&gt; as input, but the second argument (the output of this function) is another &lt;code&gt;Function&lt;/code&gt; that takes a &lt;code&gt;GlobalKTable&lt;/code&gt; as input. This second Function has another function as its output, which has an input of another &lt;code&gt;GlobalKTable&lt;/code&gt;. This third function is exhausting our inputs, and this function has a &lt;code&gt;KStream&lt;/code&gt; as its output, which will be used for the output binding. &lt;/p&gt;
&lt;p&gt;Let¡¯s look at this model from a mathematical perspective. &lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s call these three functions as &lt;code&gt;f(x)&lt;/code&gt;, &lt;code&gt;f(y)&lt;/code&gt; and &lt;code&gt;f(z)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;If we expand these functions, it will look like this: &lt;/p&gt;
&lt;p&gt;&lt;code&gt;f(x) -&amp;gt; f(y) -&amp;gt; f(z) -&amp;gt;  KStream&amp;lt;Long, EnrichedOrder&amp;gt;&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;x&lt;/strong&gt; variable stands for &lt;code&gt;KStream&amp;lt;Long, Order&amp;gt;&lt;/code&gt;, the &lt;strong&gt;y&lt;/strong&gt; variable stands for &lt;code&gt;GlobalKTable&amp;lt;Long, Customer&amp;gt;&lt;/code&gt; and the &lt;strong&gt;z&lt;/strong&gt; variable stands for &lt;code&gt;GlobalKTable&amp;lt;Long, Product&amp;gt;&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The first function &lt;code&gt;f(x)&lt;/code&gt; has the first input binding of the application (&lt;code&gt;KStream&amp;lt;Long, Order&amp;gt;&lt;/code&gt;) and its output is the function, &lt;code&gt;f(y)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The function&lt;code&gt;f(y)&lt;/code&gt; has the second input binding for the application (&lt;code&gt;GlobalKTable&amp;lt;Long, Customer&amp;gt;&lt;/code&gt;), and its output is yet another function, &lt;code&gt;f(z)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The input for the function &lt;code&gt;f(z)&lt;/code&gt; is the third input for the application (&lt;code&gt;GlobalKTable&amp;lt;Long, Product&amp;gt;&lt;/code&gt;) and its output is &lt;code&gt;KStream&amp;lt;Long, EnrichedOrder&amp;gt;&lt;/code&gt;, which is the final output binding for the application.&lt;/p&gt;
&lt;p&gt;The inputs from the three partial functions (&lt;code&gt;KStream&lt;/code&gt;, &lt;code&gt;GlobalKTable&lt;/code&gt;, &lt;code&gt;GlobalKTable&lt;/code&gt;, respectively) are available in the method body for implementing the business logic as part of the lambda expression.&lt;/p&gt;
&lt;p&gt;Bear in mind that, using function currying in Java as described above for more than a reasonable number of inputs (like three as in the above example) might cause code readability issues. Therefore, you have to carefully evaluate and decompose your application to see the appropriateness of having a larger number of input bindings in a single processor.&lt;/p&gt;&lt;h2&gt;&lt;a href="#summary" class="anchor" name="summary"&gt;&lt;/a&gt;Summary&lt;/h2&gt;
&lt;p&gt;In this blog post, we took a whirlwind tour of the various functional programming models that you can use in a Spring Cloud Stream-based Kafka Streams applications. We saw the ways in which we can use &lt;code&gt;java.util.function.Function&lt;/code&gt; (or &lt;code&gt;Consumer&lt;/code&gt; as we saw in the &lt;a href="https://spring.io/blog/2019/12/02/stream-processing-with-spring-cloud-stream-and-apache-kafka-streams-part-1-programming-model"&gt;previous blog&lt;/a&gt;), &lt;code&gt;java.util.function.BiFunction&lt;/code&gt;, and &lt;code&gt;BiConsumer.&lt;/code&gt; We also saw how multiple bindings can be supported on the outbound by using Kafka Stream¡¯s branching feature, which provides an array of &lt;code&gt;KStream&lt;/code&gt; as output. Finally, we saw the ways in which more than two input bindings can be supported through partially applied (curried) functions. In the next blog post, we will see how data deserialization and serialization are performed by the Kafka Streams binder. &lt;/p&gt;
&lt;!-- rendered by Sagan Renderer Service --&gt;</content>
  </entry>
  <entry>
    <title>Spring Framework maintenance roadmap in 2020 (including 4.3 EOL)</title>
    <link rel="alternate" href="https://spring.io/blog/2019/12/03/spring-framework-maintenance-roadmap-in-2020-including-4-3-eol" />
    <category term="releases" label="Releases" />
    <author>
      <name>Juergen Hoeller</name>
    </author>
    <id>tag:spring.io,2019-12-03:3894</id>
    <updated>2019-12-03T13:44:11Z</updated>
    <content type="html">&lt;p&gt;Dear Spring community,&lt;/p&gt;
&lt;p&gt;With &lt;a href="https://spring.io/blog/2019/12/03/spring-framework-5-2-2-and-5-1-12-available-now"&gt;Spring Framework 5.2.2 and 5.1.12 being available now&lt;/a&gt;, let me take the opportunity to provide an update on the maintenance roadmap in 2020.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Most importantly, Spring Framework 4.3.x and therefore Spring Framework 4 overall will reach its end-of-life next year: Our EOL cut-off is December 31st, 2020, with no further support on 4.3.x beyond that point. At the same time, we are also phasing out 5.0.x and 5.1.x for good.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;As for planned releases, first up is a full round in mid January: with 5.2.3 and 5.1.13 accompanied by 5.0.16 and 4.3.26. The latter are the last maintenance releases in the 5.0.x and 4.3.x lines. We may do critical patches in case of vulnerabilities but otherwise no further releases are planned in those lines until the final cut-off at the end of 2020.&lt;/p&gt;
&lt;p&gt;The 5.1.x line will receive general maintenance throughout 2020 but just with infrequent releases (~ once a quarter). The primary active branch is 5.2.x now, with frequent releases planned (~ once a month), supporting not only the current Spring Boot 2.2 generation but also the upcoming Spring Boot 2.3 (April 2020) for its entire lifetime.&lt;/p&gt;
&lt;p&gt;Last but not least, the next Spring Framework feature release will be 5.3, with GA planned for October 2020, aligned with Spring Boot 2.4. This is expected to be the last 5.x feature branch, enjoying an extended support life. We intend to wrap up all 5.x themes for 5.3, including our runtime tuning efforts (startup performance, memory consumption).&lt;/p&gt;
&lt;p&gt;&lt;em&gt;TL;DR: By the end of 2020, the only active Spring Framework branches are going to be 5.2.x and the then-new 5.3.x line (which is expected to receive long-term support, effectively superseding 4.3.x from that perspective). Please upgrade to 5.2+ at your earliest convenience.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Cheers,&lt;br/&gt;Juergen&lt;/p&gt;
&lt;p&gt;P.S.: See the &lt;a href="https://github.com/spring-projects/spring-framework/wiki/Spring-Framework-Versions#supported-versions"&gt;versions page&lt;/a&gt; for support timeframes and the &lt;a href="https://github.com/spring-projects/spring-framework/milestones"&gt;milestones page&lt;/a&gt; for release dates.&lt;/p&gt;
&lt;!-- rendered by Sagan Renderer Service --&gt;</content>
  </entry>
  <entry>
    <title>Spring Framework 5.2.2 and 5.1.12 available now</title>
    <link rel="alternate" href="https://spring.io/blog/2019/12/03/spring-framework-5-2-2-and-5-1-12-available-now" />
    <category term="releases" label="Releases" />
    <author>
      <name>Brian Clozel</name>
    </author>
    <id>tag:spring.io,2019-12-01:3890</id>
    <updated>2019-12-03T10:43:20Z</updated>
    <content type="html">&lt;p&gt;On behalf of the team and everyone who has contributed, I am pleased to announce that Spring Framework 5.2.2 and 5.1.12 are available now.&lt;/p&gt;
&lt;p&gt;The second maintenance release of the 5.2 line includes &lt;a href="https://github.com/spring-projects/spring-framework/releases/tag/v5.2.2.RELEASE"&gt;over 100 fixes and improvements&lt;/a&gt;. Spring Framework 5.1.12 &lt;a href="https://github.com/spring-projects/spring-framework/releases/tag/v5.1.12.RELEASE"&gt;includes 25 selected fixes and improvements&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;As usual, we&amp;rsquo;ll follow up shortly with corresponding Spring Boot releases (2.2.2 and 2.1.11).&lt;/p&gt;
&lt;p&gt;See you early 2020 for a full round of Spring Framework releases, from 4.3.x up to the 5.2.x generation. More on that in a future blog post, stay tuned!&lt;/p&gt;
&lt;p&gt;&lt;a href="https://projects.spring.io/spring-framework/"&gt;Project Page&lt;/a&gt; | &lt;a href="https://github.com/spring-projects/spring-framework"&gt;GitHub&lt;/a&gt; | &lt;a href="https://github.com/spring-projects/spring-framework/issues"&gt;Issues&lt;/a&gt; | &lt;a href="https://docs.spring.io/spring-framework/docs/5.2.2.RELEASE/spring-framework-reference"&gt;Documentation&lt;/a&gt;&lt;/p&gt;
&lt;!-- rendered by Sagan Renderer Service --&gt;</content>
  </entry>
  <entry>
    <title>This Week in Spring - December 3, 2019</title>
    <link rel="alternate" href="https://spring.io/blog/2019/12/03/this-week-in-spring-december-3-2019" />
    <category term="engineering" label="Engineering" />
    <author>
      <name>Josh Long</name>
    </author>
    <id>tag:spring.io,2019-12-03:3893</id>
    <updated>2019-12-03T09:38:26Z</updated>
    <content type="html">&lt;p&gt;Hi, Spring fans! Can you believe - and I can&amp;rsquo;t, by the way - that we&amp;rsquo;re already in December 2019? The last month before the new year? The last month of this &lt;em&gt;decade&lt;/em&gt;? It defies belief! I can&amp;rsquo;t even imagine how we got this far so quickly, but it&amp;rsquo;s great that we did. I started writing &lt;em&gt;This Week in Spring&lt;/em&gt; in the first week of January 2011, so we&amp;rsquo;re fast approaching 9 years of &lt;em&gt;This Week in Spring&lt;/em&gt;! &lt;/p&gt;
&lt;p&gt;As I write this I am in Toronto, Canada, for the last stop on the SpringOne Tour train for 2019. I enjoyed giving a two-hour talk introducing all sorts of stuff in the wide world of Reactive Spring yesterday. Now, I am just biding my time, preparing for my departure to Australia later today. I am off to Australia for two stops on the YOW! conference circuit. I wholely look forward to seeing everyone there! &lt;/p&gt;
&lt;p&gt;Among other things, I am using my free time to prepare &lt;a href="https://content.pivotal.io/webinars/dec-5-introducing-azure-spring-cloud-a-managed-runtime-for-spring-based-apps-webinar"&gt;my Azure Spring Cloud talk for my upcoming December 5 webinar&lt;/a&gt;. If you haven&amp;rsquo;t already registered, you should! There&amp;rsquo;s so much to talk about in this brand new offering between Microsoft Azure, Pivotal and of course the Spring team, and you shouldn&amp;rsquo;t miss it! &lt;/p&gt;
&lt;p&gt;As usual, friends, we&amp;rsquo;ve got a ton of stuff to get to so let&amp;rsquo;s dive right in! &lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href="https://spring.io/blog/2019/12/02/stream-processing-with-spring-cloud-stream-and-apache-kafka-streams-part-1-programming-model"&gt;Stream Processing with Spring Cloud Stream and Apache Kafka Streams. Part 1 - Programming Model&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href="https://spring.io/blog/2019/11/29/a-bootiful-podcast-spring-tools-lead-martin-lippert"&gt;A Bootiful Podcast: Spring Tools lead Martin Lippert&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href="https://spring.io/blog/2019/11/28/spring-cloud-hoxton-released"&gt;Spring Cloud Hoxton Released&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href="https://spring.io/blog/2019/11/27/spring-integration-aws-2-3-ga-and-spring-cloud-stream-kinesis-binder-2-0-ga-available"&gt;Spring Integration AWS 2.3 GA and Spring Cloud Stream Kinesis Binder 2.0 GA Available&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href="https://spring.io/guides/gs/spring-boot-kubernetes/"&gt;Getting Started ¡¤ Spring Boot Kubernetes&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href="https://spring.io/blog/2019/11/26/spring-cloud-open-service-broker-3-1-0-released"&gt;Spring Cloud Open Service Broker 3.1.0 Released&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;This is an interesting post about the &lt;a href="https://dzone.com/articles/rsocket-broker-use-case-in-alibaba-cloud"&gt;RSocket Broker Use Cases in Alibaba Cloud &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href="https://twitter.com/r2dbc/status/1201543303344336896?s=12"&gt;R2dbc is now GA&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href="https://howtodoinjava.com/spring-boot/role-based-security-jaxrs-annotations/"&gt;Spring Boot - Role Based Security with JAX-RS Annotations&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href="https://twitter.com/SpringData/status/1200707528163770368"&gt;https://twitter.com/SpringData/status/1200707528163770368&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href="https://blog.jetbrains.com/idea/2019/11/tutorial-reactive-spring-boot-displaying-reactive-data"&gt;Tutorial: Reactive Spring Boot Part 6 ? Displaying Reactive Data | IntelliJ IDEA Blog&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href="https://github.com/making/rsc"&gt;making/rsc: RSocket Client CLI (RSC) that aims to be a curl for RSocket&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Baeldung have updated their &lt;a href="https://www.baeldung.com/spring-autowire"&gt;guide to Spring&amp;rsquo;s &lt;code&gt;@Autowired&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href="https://allegro.tech/2019/07/migrating-microservice-to-spring-webflux.html"&gt;Migrating a microservice to Spring WebFlux ¡¤ allegro.tech&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;I like this (French-language) article on moving from Spring Cloud Netflix to the newer alternatives in Spring Cloud today. &lt;a href="https://javaetmoi.com/2019/11/desendettement-de-spring-cloud-netflix/"&gt;D?sendettement de Spring Cloud Netflix &lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- rendered by Sagan Renderer Service --&gt;</content>
  </entry>
  <entry>
    <title>Stream Processing with Spring Cloud Stream and Apache Kafka Streams. Part 1 - Programming Model</title>
    <link rel="alternate" href="https://spring.io/blog/2019/12/02/stream-processing-with-spring-cloud-stream-and-apache-kafka-streams-part-1-programming-model" />
    <category term="engineering" label="Engineering" />
    <author>
      <name>Soby Chacko</name>
    </author>
    <id>tag:spring.io,2019-12-02:3891</id>
    <updated>2019-12-02T17:08:00Z</updated>
    <content type="html">&lt;p&gt;This is the first in a series of blog posts in which we will look at how stream processing applications are written using Spring Cloud Stream and Kafka Streams. &lt;/p&gt;
&lt;p&gt;The &lt;b&gt;Spring Cloud Stream Horsham release (3.0.0)&lt;/b&gt; introduces several changes to the way applications can leverage Apache Kafka using the binders for Kafka and Kafka Streams.&lt;br/&gt;One of the major enhancements that this release brings to the table is first class support for writing apps by using a fully functional programming paradigm. This blog post gives an introduction to how this functional programming model can be used to develop stream processing applications with Spring Cloud Stream and Kafka Streams. In the subsequent blog posts in this series, we will look into more details. &lt;/p&gt;&lt;h2&gt;&lt;a href="#how-many-types-of-kafka-binders-are-there-under-spring-cloud-stream" class="anchor" name="how-many-types-of-kafka-binders-are-there-under-spring-cloud-stream"&gt;&lt;/a&gt;How many types of Kafka binders are there under Spring Cloud Stream?&lt;/h2&gt;
&lt;p&gt;This is often a confusing question: Which binder should I use if I want to write applications based on Apache Kafka. Spring Cloud Stream provides two separate binders for Kafka - &lt;b&gt;spring-cloud-stream-binder-kafka&lt;/b&gt; and &lt;b&gt;spring-cloud-stream-binder-kafka-streams&lt;/b&gt;. As their names indicate, the first one is the one that you want to use if you want to write standard event-driven applications in which you want to use normal Kafka producers and consumers. On the other hand, if you want to develop stream processing applications with the Kafka Streams library, use the second binder. Once again, in this blog post, we will focus on the second binder for Kafka Streams. &lt;/p&gt;
&lt;p&gt;One general note about this blog series. This is mainly looking at the touchpoints between Spring Cloud Stream and Kafka Streams and does not go into the details of Kafka Streams itself. In order to write non-trivial stream processing applications that use Kafka Streams, a deeper understanding of Kafka Streams library is highly recommended. This series only stays at the periphery on the actual Kafka Streams library and mainly focuses on how you can interact with it from a Spring Cloud Stream vantage point. &lt;/p&gt;&lt;h2&gt;&lt;a href="#bootstrapping-a-spring-cloud-stream-kafka-streams-application" class="anchor" name="bootstrapping-a-spring-cloud-stream-kafka-streams-application"&gt;&lt;/a&gt;Bootstrapping a Spring Cloud Stream Kafka Streams application&lt;/h2&gt;
&lt;p&gt;At the heart of it, all Spring Cloud Stream applications are Spring Boot applications. In order to bootstrap a new project, go to the &lt;a href="https://start.spring.io"&gt;Spring Initializr&lt;/a&gt; and then create a new project. Select &lt;b&gt;¡°Cloud Stream¡±&lt;/b&gt; and &lt;b&gt;¡°Spring for Apache Kafka Streams¡±&lt;/b&gt; as dependencies. This will generate a project with all the components that you need to start developing the application. Here is a screenshot from the initializr with the basic dependencies selected.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/spring-cloud/spring-cloud-stream-binder-kafka/master/docs/src/main/asciidoc/images/spring-initializr-kafka-streams.png" alt="spring-initializr-kafka-streams" /&gt;&lt;/p&gt;&lt;h2&gt;&lt;a href="#show-me-a-simple-example-of-how-i-can-use-spring-cloud-stream-to-write-a-quick-kafka-streams-application" class="anchor" name="show-me-a-simple-example-of-how-i-can-use-spring-cloud-stream-to-write-a-quick-kafka-streams-application"&gt;&lt;/a&gt;Show me a simple example of how I can use Spring Cloud Stream to write a quick Kafka Streams application&lt;/h2&gt;
&lt;p&gt;Here is a very basic, but functional, Kafka Streams application that is written by using Spring Cloud Stream¡¯s functional programming support:&lt;/p&gt;
&lt;pre&gt;&lt;code class="prettyprint"&gt;@SpringBootApplication&#xD;
public class SimpleConsumerApplication {&#xD;
&#xD;
   @Bean&#xD;
   public java.util.function.Consumer&amp;lt;KStream&amp;lt;String, String&amp;gt;&amp;gt; process() {&#xD;
&#xD;
       return input -&amp;gt;&#xD;
               input.foreach((key, value) -&amp;gt; {&#xD;
                   System.out.println(&amp;quot;Key: &amp;quot; + key + &amp;quot; Value: &amp;quot; + value);&#xD;
               });&#xD;
   }&#xD;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As you can see, this is a very trivial application that just prints to standard output but is, nonetheless, a full-blown Kafka Streams application. At the outer layer, we indicate that this is a boot application by using the &lt;code&gt;@SpringBootApplication&lt;/code&gt; annotation. Then we provide a &lt;code&gt;java.util.function.Consumer&lt;/code&gt; bean where we encapsulate our application¡¯s logic through a lambda expression. The consumer takes a &lt;b&gt;KStream&lt;/b&gt; as its input with both the key and the value represented as String types. &lt;/p&gt;
&lt;p&gt;That¡¯s it. You can run this application against a Kafka broker and see it in action. Behind the scenes, the Kafka Streams binder for Spring Cloud Stream will convert this into a proper Kafka Streams application with a &lt;code&gt;StreamsBuilder&lt;/code&gt;, Kafka Streams topology, and so on. One of the prime tenets for Spring Cloud Stream is hiding the complexity and boilerplate away from the user so that the application developer can focus on the business issue at hand. Binder will take care of creating the Kafka Streams topology, connecting to a Kafka Cluster, binding to a topic and consuming data from that Kafka topic, which is bound as &lt;b&gt;KStream&lt;/b&gt; in this case. Usually, it is the responsibility of the application developer to do all these things if they are not using a framework such as Spring Cloud Stream. &lt;/p&gt;&lt;h2&gt;&lt;a href="#wait-a-minute-are-you-sure-this-is-going-to-work" class="anchor" name="wait-a-minute-are-you-sure-this-is-going-to-work"&gt;&lt;/a&gt;Wait a minute, Are you sure this is going to work?&lt;/h2&gt;
&lt;p&gt;If you know Kafka Streams internals, you might be wondering if what is presented above will work or not. We haven¡¯t provided a number of basic things that Kafka Streams requires (such as the cluster information, application id, the topic to consume, Serdes to use, and so on). The short answer is that this is going to work without providing a single configuration property. This is because the binder will use a lot of reasonable defaults and make opinions as to what topics to consume from and so on. Nevertheless, for production use, we recommend providing all the applicable properties if the defaults used by the binder do not make sense.&lt;/p&gt;
&lt;p&gt;Let¡¯s look at some of these basic things that Kafka Streams requires and how the binder provides default values for them. &lt;/p&gt;&lt;h2&gt;&lt;a href="#cluster-information" class="anchor" name="cluster-information"&gt;&lt;/a&gt;Cluster information&lt;/h2&gt;
&lt;p&gt;By default, the binder will try to connect to a cluster that is running on &lt;b&gt;localhost:9092&lt;/b&gt;. If that is not the case, you can override that by using configuration properties available through Spring Cloud Stream. See the &lt;a href="https://cloud.spring.io/spring-cloud-static/spring-cloud-stream-binder-kafka/3.0.0.RELEASE/reference/html/spring-cloud-stream-binder-kafka.html#_setting_up_bootstrap_server_configuration"&gt;Spring Cloud Stream Reference Guide&lt;/a&gt;.&lt;/p&gt;&lt;h2&gt;&lt;a href="#application-id" class="anchor" name="application-id"&gt;&lt;/a&gt;Application ID&lt;/h2&gt;
&lt;p&gt;In a Kafka Streams application, application.id is a mandatory field. Without it, you cannot start a Kafka Streams application. By default, the binder will generate an application ID and assign it to the processor. It uses the function bean name as a prefix. For e.g, if you have a consumer as above, the binder will generate the application ID as process-applicationId. You can override this using the strategies outlined here. See the &lt;a href="https://cloud.spring.io/spring-cloud-static/spring-cloud-stream-binder-kafka/3.0.0.RELEASE/reference/html/spring-cloud-stream-binder-kafka.html#_kafka_streams_application_id"&gt;Spring Cloud Stream Reference Guide&lt;/a&gt;.&lt;/p&gt;&lt;h2&gt;&lt;a href="#topic-to-consume-from" class="anchor" name="topic-to-consume-from"&gt;&lt;/a&gt;Topic to consume from&lt;/h2&gt;
&lt;p&gt;For the above processor, you can provide the topic to consumes, as follows&lt;/p&gt;
&lt;p&gt;&lt;code&gt;spring.cloud.stream.bindings.process-in-0.destination: my-input-topic&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;In this case, we are saying that, for the function bean (process) and its first input (in-0), it shall be bound to a Kafka topic named &lt;b&gt;my-input-topic&lt;/b&gt;. If you don¡¯t provide an explicit destination like this, the binder assumes that you are using a topic that is the same as the binding name (&lt;b&gt;process-in-0&lt;/b&gt;, in this case). &lt;/p&gt;&lt;h2&gt;&lt;a href="#serialization-and-deserialization-serdes" class="anchor" name="serialization-and-deserialization-serdes"&gt;&lt;/a&gt;Serialization and Deserialization (Serdes)&lt;/h2&gt;
&lt;p&gt;Kafka Streams uses a special class called Serde to deal with data marshaling. It is essentially a wrapper around a deserializer on the inbound and a serializer on the outbound. Normally, you have to tell Kafka Streams what Serde to use for each consumer. Binder, however, infers this information by using the parametric types provided as part of Kafka Streams. For example, in the case of &lt;b&gt;KStream&amp;lt;String, String&amp;gt;&lt;/b&gt;, the binder assumes that it needs to use String deserializers. As always, you can override these in a number of ways. &lt;Provide links to the docs&gt;. We have an entire blog post in this series coming up that is dedicated to this topic.&lt;/p&gt;&lt;h2&gt;&lt;a href="#can-i-have-multiple-processors-in-a-single-boot-application" class="anchor" name="can-i-have-multiple-processors-in-a-single-boot-application"&gt;&lt;/a&gt;Can I have multiple processors in a single Boot application?&lt;/h2&gt;
&lt;p&gt;Yes, you can. Spring Cloud Stream binder for Kafka Streams will make it easy to provide multiple processors expressed as &lt;b&gt;java.util.function.Function&lt;/b&gt; or &lt;b&gt;java.util.function.Consumer&lt;/b&gt; beans within a single application. The binder will isolate each such processor to its own application ID and StreamsBuilder. It ensures that there won¡¯t be any interference with each other. From a Kafka Streams angle, they are multiple processors with their own dedicated topology. Although this is a legitimate use-case when it comes to things like testing and trying out something really quick, having several processors within a single application can have the potential of making it a monolith that is harder to maintain. &lt;/p&gt;&lt;h2&gt;&lt;a href="#summary" class="anchor" name="summary"&gt;&lt;/a&gt;Summary&lt;/h2&gt;
&lt;p&gt;In this blog post, we saw a quick introduction to how Spring Cloud Stream¡¯s functional programming support can be used to write stream processing applications that use Kafka Streams. We saw that the binder takes care of a lot of infrastructure and configuration details, which lets you focus on the business logic at hand. In the next blog post, we are going to further explore this programming model to see how more non-trivial stream processing applications are developed with Spring Cloud Stream and Kafka Streams. &lt;/p&gt;
&lt;!-- rendered by Sagan Renderer Service --&gt;</content>
  </entry>
  <entry>
    <title>A Bootiful Podcast: Spring Tools lead Martin Lippert</title>
    <link rel="alternate" href="https://spring.io/blog/2019/11/29/a-bootiful-podcast-spring-tools-lead-martin-lippert" />
    <category term="engineering" label="Engineering" />
    <author>
      <name>Josh Long</name>
    </author>
    <id>tag:spring.io,2019-11-29:3889</id>
    <updated>2019-11-29T07:04:39Z</updated>
    <content type="html">&lt;p&gt;Hi, Spring fans! In today&amp;rsquo;s episode Josh Long (&lt;a href="http://twitter.com/starbuxman"&gt;@starbuxman&lt;/a&gt;) talks to Spring Tools lead Martin Lippert (&lt;a href="http://twitter.com/martinlippert"&gt;@martinlippert&lt;/a&gt;) about his time at Pivotal, and on the Spring team, his work on Spring Tools, and his work on language servers that now serve as the foundational integration for Spring users using Microsoft&amp;rsquo;s Visual Studio Code, emacs and Atom, among other things.&lt;/p&gt;
&lt;p&gt;Thanks, dear listener, and Happy Thanksgiving!&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href="http://spring.io/tools"&gt;Spring Tools&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href="http://spring.io/team/mlippert"&gt;Martin Lippert&amp;rsquo;s profile page&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Thanksgiving"&gt;the Wikipedia description for Thanksgiving&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;iframe width="100%" height="300" scrolling="no" frameborder="no" allow="autoplay" src="https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/720162076&amp;color=%23ff5500&amp;auto_play=false&amp;hide_related=false&amp;show_comments=true&amp;show_user=true&amp;show_reposts=false&amp;show_teaser=true&amp;visual=true"&gt;&lt;/iframe&gt;
&lt;!-- rendered by Sagan Renderer Service --&gt;</content>
  </entry>
  <entry>
    <title>Spring Cloud Hoxton Released</title>
    <link rel="alternate" href="https://spring.io/blog/2019/11/28/spring-cloud-hoxton-released" />
    <category term="releases" label="Releases" />
    <author>
      <name>Ryan Baxter</name>
    </author>
    <id>tag:spring.io,2019-11-27:3888</id>
    <updated>2019-11-28T08:35:16Z</updated>
    <content type="html">&lt;p&gt;On behalf of the community, I am pleased to announce that the General Availability (RELEASE) of the &lt;a href="https://cloud.spring.io"&gt;Spring Cloud Hoxton&lt;/a&gt; Release Train is available today. The release can be found in &lt;a href="https://repo1.maven.org/maven2/org/springframework/cloud/spring-cloud-dependencies/Hoxton.RELEASE/"&gt;Maven Central&lt;/a&gt;. You can check out the Hoxton &lt;a href="https://github.com/spring-projects/spring-cloud/wiki/Spring-Cloud-Hoxton-Release-Notes"&gt;release notes for more information&lt;/a&gt;.&lt;/p&gt;&lt;h2&gt;&lt;a href="#notable-changes-in-the-hoxton-release-train" class="anchor" name="notable-changes-in-the-hoxton-release-train"&gt;&lt;/a&gt;Notable Changes in the Hoxton Release Train&lt;/h2&gt;
&lt;p&gt;Spring Cloud Hoxton.RELEASE is based on Spring Boot 2.2.1.RELEASE.&lt;/p&gt;&lt;h3&gt;&lt;a href="#documentation-changes" class="anchor" name="documentation-changes"&gt;&lt;/a&gt;Documentation Changes&lt;/h3&gt;
&lt;p&gt;The Hoxton.RELEASE docs have a new &lt;a href="https://cloud.spring.io/spring-cloud-static/Hoxton.RELEASE/reference/html/spring-cloud.html"&gt;landing page&lt;/a&gt;, new theme and a &lt;a href="https://cloud.spring.io/spring-cloud-static/Hoxton.RELEASE/reference/html/documentation-overview.html#contract-documentation"&gt;single-page, multi-page and a pdf version&lt;/a&gt;.The landing page will link you to the documentation for the specific project you are interested in. We hope you find that the new documentation structure easier to consume.&lt;/p&gt;&lt;h3&gt;&lt;a href="#new-load-balancer-implementations" class="anchor" name="new-load-balancer-implementations"&gt;&lt;/a&gt;New Load Balancer Implementations&lt;/h3&gt;
&lt;p&gt;Spring Cloud Hoxton.RELEASE is the first release containing both blocking and non-blocking load balancer client implementations as an alternative to Netflix Ribbon which has entered maintenance mode.&lt;/p&gt;
&lt;p&gt;To use the new &lt;code&gt;BlockingLoadBalancerClient&lt;/code&gt; with a &lt;code&gt;RestTemplate&lt;/code&gt; you will need to include &lt;code&gt;org.springframework.cloud:spring-cloud-loadbalancer&lt;/code&gt; on your application&amp;rsquo;s classpath. The same dependency can be used in a reactive application when using &lt;code&gt;@LoadBalanced WebClient.Builder&lt;/code&gt; - the only difference is that Spring Cloud will auto-configure a &lt;code&gt;ReactorLoadBalancerExchangeFilterFunction&lt;/code&gt; instance. See the &lt;a href="https://cloud.spring.io/spring-cloud-static/spring-cloud-commons/2.2.0.M2/reference/html/#_spring_resttemplate_as_a_load_balancer_client"&gt;documentation&lt;/a&gt; for additional information. The new &lt;code&gt;ReactorLoadBalancerExchangeFilterFunction&lt;/code&gt; can also be autowired and passed directly to &lt;code&gt;WebClient.Builder&lt;/code&gt; (see the &lt;a href="https://cloud.spring.io/spring-cloud-commons/reference/html/#webflux-with-reactive-loadbalancer"&gt;documentation&lt;/a&gt;). For all these features, &lt;a href="https://projectreactor.io/"&gt;Project Reactor&lt;/a&gt;-based &lt;code&gt;RoundRobinLoadBalancer&lt;/code&gt; is used underneath.&lt;/p&gt;&lt;h3&gt;&lt;a href="#spring-cloud-netflix" class="anchor" name="spring-cloud-netflix"&gt;&lt;/a&gt;Spring Cloud Netflix&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Support was added for the new &lt;code&gt;ReactiveDiscoveryClient&lt;/code&gt; and the new Spring Cloud Circuit Breaker API implementation for Hystrix.&lt;/li&gt;
  &lt;li&gt;Added &lt;a href="https://cloud.spring.io/spring-cloud-static/spring-cloud-netflix/2.2.0.RC2/reference/html/#disabling-spring-cloud-circuit-breaker-hystrix"&gt;property&lt;/a&gt; to disable Spring Cloud CircuitBreaker Hystrix auto-configuration&lt;/li&gt;
&lt;/ul&gt;&lt;h3&gt;&lt;a href="#spring-cloud-cloudfoundry" class="anchor" name="spring-cloud-cloudfoundry"&gt;&lt;/a&gt;Spring Cloud Cloudfoundry&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Support was added for the new &lt;code&gt;ReactiveDiscoveryClient&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;&lt;h3&gt;&lt;a href="#spring-cloud-bus" class="anchor" name="spring-cloud-bus"&gt;&lt;/a&gt;Spring Cloud Bus&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Documentation updates&lt;/li&gt;
&lt;/ul&gt;&lt;h3&gt;&lt;a href="#spring-cloud-vault" class="anchor" name="spring-cloud-vault"&gt;&lt;/a&gt;Spring Cloud Vault&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Applications running in the Pivotal Application Service (former PCF) can leverage container identity to authenticate using Vault&amp;rsquo;s PCF Authentication support.&lt;/li&gt;
  &lt;li&gt;Support for Vault namespaces (Vault Enterprise feature) using the &lt;code&gt;X-Vault-Namespace&lt;/code&gt; header.&lt;/li&gt;
&lt;/ul&gt;&lt;h3&gt;&lt;a href="#spring-cloud-kubernetes" class="anchor" name="spring-cloud-kubernetes"&gt;&lt;/a&gt;Spring Cloud Kubernetes&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Support was added for the new &lt;code&gt;ReactiveDiscoveryClient&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;&lt;h3&gt;&lt;a href="#spring-cloud-contract" class="anchor" name="spring-cloud-contract"&gt;&lt;/a&gt;Spring Cloud Contract&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Full documentation rewrite&lt;/li&gt;
  &lt;li&gt;Major test class generation refactoring&lt;/li&gt;
  &lt;li&gt;A lot of rewrite from Groovy to Java&lt;/li&gt;
  &lt;li&gt;Added support for writing contracts in Kotlin and Java&lt;/li&gt;
  &lt;li&gt;Added &lt;code&gt;inProgress&lt;/code&gt; flag to the contract DSL and runtime stub generation&lt;/li&gt;
  &lt;li&gt;Added TestNG support for generated tests&lt;/li&gt;
  &lt;li&gt;Numerous library version increments (including Groovy, WireMock and Pact)&lt;/li&gt;
&lt;/ul&gt;&lt;h3&gt;&lt;a href="#spring-cloud-consul" class="anchor" name="spring-cloud-consul"&gt;&lt;/a&gt;Spring Cloud Consul&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Support was added for the new &lt;code&gt;ReactiveDiscoveryClient&lt;/code&gt; and for Consul&amp;rsquo;s consistency mode.&lt;/li&gt;
&lt;/ul&gt;&lt;h3&gt;&lt;a href="#spring-cloud-config" class="anchor" name="spring-cloud-config"&gt;&lt;/a&gt;Spring Cloud Config&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;An Environment Repository supporting AWS S3.&lt;/li&gt;
  &lt;li&gt;Added the ability to &lt;a href="https://cloud.spring.io/spring-cloud-static/spring-cloud-config/2.2.0.RC2/reference/html/#_decrpyting_plain_text"&gt;decrypt properties in plain text&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;h3&gt;&lt;a href="#spring-cloud-gcp" class="anchor" name="spring-cloud-gcp"&gt;&lt;/a&gt;Spring Cloud Gcp&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;BigQuery module added&lt;/li&gt;
  &lt;li&gt;Created a separate starter for Cloud Foundry: spring-cloud-gcp-starter-cloudfoundry&lt;/li&gt;
  &lt;li&gt;You can check out the &lt;a href="https://github.com/spring-cloud/spring-cloud-gcp/blob/master/CHANGELOG.adoc#120release-2019-11-26"&gt;changelog document&lt;/a&gt; for more information&lt;/li&gt;
&lt;/ul&gt;&lt;h3&gt;&lt;a href="#spring-cloud-stream" class="anchor" name="spring-cloud-stream"&gt;&lt;/a&gt;Spring Cloud Stream&lt;/h3&gt;
&lt;p&gt;With this new Horsham.RELEASE (3.0.0) we begin our journey from annotation-driven to a significantly simpler functional approach. We have published a series of posts explaining and justifying this move:&lt;br/&gt;- &lt;em&gt;&lt;a href="https://spring.io/blog/2019/10/14/spring-cloud-stream-demystified-and-simplified"&gt;Spring Cloud Stream - demystified and simplified&lt;/a&gt;&lt;/em&gt;&lt;br/&gt;- &lt;em&gt;&lt;a href="https://spring.io/blog/2019/10/17/spring-cloud-stream-functional-and-reactive"&gt;Spring Cloud Stream - functional and reactive&lt;/a&gt;&lt;/em&gt;&lt;br/&gt;- &lt;em&gt;&lt;a href="https://spring.io/blog/2019/10/25/spring-cloud-stream-and-spring-integration"&gt;Spring Cloud Stream - and Spring Integration&lt;/a&gt;&lt;/em&gt;&lt;br/&gt;- &lt;em&gt;&lt;a href="https://spring.io/blog/2019/10/31/spring-cloud-stream-event-routing"&gt;Spring Cloud Stream - Event Routing&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;&lt;h3&gt;&lt;a href="#spring-cloud-commons" class="anchor" name="spring-cloud-commons"&gt;&lt;/a&gt;Spring Cloud Commons&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;We have introduced new blocking and non-blocking load balancer implementations as an&lt;br/&gt; alternative to Netflix Ribbon which has entered maintenance mode.&lt;/li&gt;
&lt;/ul&gt;&lt;h3&gt;&lt;a href="#spring-cloud-openfeign" class="anchor" name="spring-cloud-openfeign"&gt;&lt;/a&gt;Spring Cloud Openfeign&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;OpenFeign was updated to 10.4.0.&lt;/li&gt;
  &lt;li&gt;&lt;a href="https://github.com/spring-cloud/spring-cloud-openfeign/issues/177"&gt;Support for Spring Cloud LoadBalancer&lt;/a&gt; has been added&lt;/li&gt;
&lt;/ul&gt;&lt;h3&gt;&lt;a href="#spring-cloud-task" class="anchor" name="spring-cloud-task"&gt;&lt;/a&gt;Spring Cloud Task&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Micrometer support&lt;/li&gt;
  &lt;li&gt;Updated documentation with improved format&lt;/li&gt;
  &lt;li&gt;Task apps launched when using Spring Batch partitioning now have external-execution-id populated&lt;/li&gt;
&lt;/ul&gt;&lt;h3&gt;&lt;a href="#spring-cloud-sleuth" class="anchor" name="spring-cloud-sleuth"&gt;&lt;/a&gt;Spring Cloud Sleuth&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Added support for latest Brave (includes messaging sampling)&lt;/li&gt;
  &lt;li&gt;Added an option for &lt;code&gt;onLastOperator&lt;/code&gt; Reactor tracing for improved performance&lt;/li&gt;
  &lt;li&gt;Added Redis tracing&lt;/li&gt;
  &lt;li&gt;Set default sampler to rate-limited sampler&lt;/li&gt;
  &lt;li&gt;Added support for AWS SQS tracing&lt;/li&gt;
  &lt;li&gt;Added support for Quartz tracing&lt;/li&gt;
  &lt;li&gt;Added in-process propagation mechanism&lt;/li&gt;
  &lt;li&gt;Defaults to Micrometer metrics for Zipkin reporting&lt;/li&gt;
&lt;/ul&gt;&lt;h3&gt;&lt;a href="#spring-cloud-aws" class="anchor" name="spring-cloud-aws"&gt;&lt;/a&gt;Spring Cloud Aws&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Bug fixes&lt;/li&gt;
&lt;/ul&gt;&lt;h3&gt;&lt;a href="#spring-cloud-zookeeper" class="anchor" name="spring-cloud-zookeeper"&gt;&lt;/a&gt;Spring Cloud Zookeeper&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Support was added for the new &lt;code&gt;ReactiveDiscoveryClient&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;&lt;h3&gt;&lt;a href="#spring-cloud-security" class="anchor" name="spring-cloud-security"&gt;&lt;/a&gt;Spring Cloud Security&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Bug fixes&lt;/li&gt;
&lt;/ul&gt;&lt;h3&gt;&lt;a href="#spring-cloud-circuitbreaker" class="anchor" name="spring-cloud-circuitbreaker"&gt;&lt;/a&gt;Spring Cloud Circuitbreaker&lt;/h3&gt;
&lt;p&gt;We welcome Spring Cloud Circuit Breaker as a new project under the Spring Cloud release train. This project provides an abstraction API for adding circuit breakers to your application. At the time of this blog post, there are four supported implementations:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Resilience4j&lt;/li&gt;
  &lt;li&gt;Spring Retry&lt;/li&gt;
  &lt;li&gt;Hystrix (in &lt;a href="https://github.com/spring-cloud/spring-cloud-netflix/blob/master/spring-cloud-netflix-hystrix/src/main/java/org/springframework/cloud/netflix/hystrix/HystrixCircuitBreaker.java"&gt;spring-cloud-netflix&lt;/a&gt;)&lt;/li&gt;
  &lt;li&gt;Sentinel (in &lt;a href="https://github.com/alibaba/spring-cloud-alibaba/tree/master/spring-cloud-alibaba-sentinel"&gt;spring-cloud-alibaba&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;See the &lt;a href="https://spring.io/blog/2019/04/16/introducing-spring-cloud-circuit-breaker"&gt;annoucement blog post&lt;/a&gt; for more information.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Added auto-configuration to collect circuit breaker metrics when using Resilience4J (&lt;a href="https://github.com/spring-cloud/spring-cloud-circuitbreaker/issues/47"&gt;#47&lt;/a&gt;)&lt;/li&gt;
  &lt;li&gt;Upgrade to Resilience4J 1.1.0&lt;/li&gt;
  &lt;li&gt;&lt;a href="https://github.com/spring-cloud/spring-cloud-circuitbreaker/commit/aacf4d35183de568e560f0b9d6e5b54fdd680ecc"&gt;Added property to disable Resilience4J auto-configuration&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;h3&gt;&lt;a href="#spring-cloud-function" class="anchor" name="spring-cloud-function"&gt;&lt;/a&gt;Spring Cloud Function&lt;/h3&gt;
&lt;p&gt;Lots of new features such as:&lt;br/&gt;- &lt;em&gt;Transparent type conversion&lt;/em&gt;&lt;br/&gt;- &lt;em&gt;Function Routing&lt;/em&gt;&lt;br/&gt;- &lt;em&gt;Function arity&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;More details in our &lt;a href="https://spring.io/blog/2019/11/25/announcing-the-release-of-spring-cloud-function-3-0-0-release"&gt;release announcement&lt;/a&gt;.&lt;/p&gt;&lt;h3&gt;&lt;a href="#spring-cloud-gateway" class="anchor" name="spring-cloud-gateway"&gt;&lt;/a&gt;Spring Cloud Gateway&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Support was added for using the new &lt;code&gt;ReactiveLoadBalancer&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;RSocket modules were moved to their own &lt;a href="https://github.com/spring-cloud-incubator/spring-cloud-rsocket"&gt;project&lt;/a&gt; in the Spring Cloud Incubator organization&lt;/li&gt;
  &lt;li&gt;Added a &lt;a href="https://cloud.spring.io/spring-cloud-static/spring-cloud-gateway/2.2.0.RC2/reference/html/#spring-cloud-circuitbreaker-filter-factory"&gt;filter&lt;/a&gt; which uses the new Spring Cloud CircuitBreaker library to provide circuit breakers to routes&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The following modules were updated as part of Hoxton.RELEASE:&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Module &lt;/th&gt;
      &lt;th&gt;Version &lt;/th&gt;
      &lt;th&gt;Issues&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Spring Cloud Netflix &lt;/td&gt;
      &lt;td&gt;2.2.0.RELEASE &lt;/td&gt;
      &lt;td&gt;&amp;nbsp;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Spring Cloud Starter Parent &lt;/td&gt;
      &lt;td&gt;Hoxton.RELEASE &lt;/td&gt;
      &lt;td&gt;&amp;nbsp;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Spring Cloud Dependencies Parent &lt;/td&gt;
      &lt;td&gt;2.2.0.RELEASE &lt;/td&gt;
      &lt;td&gt;&amp;nbsp;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Spring Cloud Dependencies &lt;/td&gt;
      &lt;td&gt;Hoxton.RELEASE &lt;/td&gt;
      &lt;td&gt;&amp;nbsp;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Spring Cloud Cloudfoundry &lt;/td&gt;
      &lt;td&gt;2.2.0.RELEASE &lt;/td&gt;
      &lt;td&gt;&amp;nbsp;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Spring Cloud Cli &lt;/td&gt;
      &lt;td&gt;2.2.0.RELEASE &lt;/td&gt;
      &lt;td&gt;(&lt;a href="https://github.com/spring-cloud/spring-cloud-cli/milestone/22?closed=1"&gt;issues&lt;/a&gt;)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Spring Cloud Bus &lt;/td&gt;
      &lt;td&gt;2.2.0.RELEASE &lt;/td&gt;
      &lt;td&gt;&amp;nbsp;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Spring Cloud Vault &lt;/td&gt;
      &lt;td&gt;2.2.0.RELEASE &lt;/td&gt;
      &lt;td&gt;(&lt;a href="https://github.com/spring-cloud/spring-cloud-vault/milestone/35?closed=1"&gt;issues&lt;/a&gt;)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Spring Cloud Kubernetes &lt;/td&gt;
      &lt;td&gt;1.1.0.RELEASE &lt;/td&gt;
      &lt;td&gt;&amp;nbsp;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Spring Cloud Contract &lt;/td&gt;
      &lt;td&gt;2.2.0.RELEASE &lt;/td&gt;
      &lt;td&gt;(&lt;a href="https://github.com/spring-cloud/spring-cloud-contract/milestone/59?closed=1"&gt;issues&lt;/a&gt;)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Spring Cloud Consul &lt;/td&gt;
      &lt;td&gt;2.2.0.RELEASE &lt;/td&gt;
      &lt;td&gt;&amp;nbsp;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Spring Cloud Release &lt;/td&gt;
      &lt;td&gt;Hoxton.RELEASE &lt;/td&gt;
      &lt;td&gt;&amp;nbsp;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Spring Cloud Build &lt;/td&gt;
      &lt;td&gt;2.2.0.RELEASE &lt;/td&gt;
      &lt;td&gt;(&lt;a href="https://github.com/spring-cloud/spring-cloud-build/milestone/27?closed=1"&gt;issues&lt;/a&gt;)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Spring Cloud Config &lt;/td&gt;
      &lt;td&gt;2.2.0.RELEASE &lt;/td&gt;
      &lt;td&gt;&amp;nbsp;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Spring Cloud &lt;/td&gt;
      &lt;td&gt;Hoxton.RELEASE &lt;/td&gt;
      &lt;td&gt;&amp;nbsp;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Spring Cloud Gcp &lt;/td&gt;
      &lt;td&gt;1.2.0.RELEASE &lt;/td&gt;
      &lt;td&gt;&amp;nbsp;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Spring Cloud Stream &lt;/td&gt;
      &lt;td&gt;Horsham.RELEASE &lt;/td&gt;
      &lt;td&gt;(&lt;a href="https://github.com/spring-cloud/spring-cloud-stream/milestone/67?closed=1"&gt;issues&lt;/a&gt;)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Spring Cloud Commons &lt;/td&gt;
      &lt;td&gt;2.2.0.RELEASE &lt;/td&gt;
      &lt;td&gt;(&lt;a href="https://github.com/spring-cloud/spring-cloud-commons/milestone/68?closed=1"&gt;issues&lt;/a&gt;)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Spring Cloud Starter &lt;/td&gt;
      &lt;td&gt;Hoxton.RELEASE &lt;/td&gt;
      &lt;td&gt;&amp;nbsp;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Spring Cloud Openfeign &lt;/td&gt;
      &lt;td&gt;2.2.0.RELEASE &lt;/td&gt;
      &lt;td&gt;&amp;nbsp;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Spring Cloud Task &lt;/td&gt;
      &lt;td&gt;2.2.1.RELEASE &lt;/td&gt;
      &lt;td&gt;&amp;nbsp;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Spring Cloud Sleuth &lt;/td&gt;
      &lt;td&gt;2.2.0.RELEASE &lt;/td&gt;
      &lt;td&gt;(&lt;a href="https://github.com/spring-cloud/spring-cloud-sleuth/milestone/70?closed=1"&gt;issues&lt;/a&gt;)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Spring Cloud Aws &lt;/td&gt;
      &lt;td&gt;2.2.0.RELEASE &lt;/td&gt;
      &lt;td&gt;&amp;nbsp;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Spring Cloud Zookeeper &lt;/td&gt;
      &lt;td&gt;2.2.0.RELEASE &lt;/td&gt;
      &lt;td&gt;(&lt;a href="https://github.com/spring-cloud/spring-cloud-zookeeper/milestone/27?closed=1"&gt;issues&lt;/a&gt;)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Spring Cloud Security &lt;/td&gt;
      &lt;td&gt;2.2.0.RELEASE &lt;/td&gt;
      &lt;td&gt;&amp;nbsp;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Spring Cloud Circuitbreaker &lt;/td&gt;
      &lt;td&gt;1.0.0.RELEASE &lt;/td&gt;
      &lt;td&gt;(&lt;a href="https://github.com/spring-cloud/spring-cloud-circuitbreaker/milestone/1?closed=1"&gt;issues&lt;/a&gt;)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Spring Cloud Function &lt;/td&gt;
      &lt;td&gt;3.0.0.RELEASE &lt;/td&gt;
      &lt;td&gt;(&lt;a href="https://github.com/spring-cloud/spring-cloud-function/milestone/26?closed=1"&gt;issues&lt;/a&gt;)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Spring Cloud Gateway &lt;/td&gt;
      &lt;td&gt;2.2.0.RELEASE &lt;/td&gt;
      &lt;td&gt;(&lt;a href="https://github.com/spring-cloud/spring-cloud-gateway/milestone/32?closed=1"&gt;issues&lt;/a&gt;)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;As always, we welcome feedback on &lt;a href="https://github.com/spring-cloud/"&gt;GitHub&lt;/a&gt;, on &lt;a href="https://gitter.im/spring-cloud/spring-cloud"&gt;Gitter&lt;/a&gt;, on &lt;a href="https://stackoverflow.com/questions/tagged/spring-cloud"&gt;Stack Overflow&lt;/a&gt;, or on &lt;a href="https://twitter.com/SpringCloud"&gt;Twitter&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To get started with Maven with a BOM (dependency management only):&lt;/p&gt;
&lt;pre&gt;&lt;code class="prettyprint"&gt;&lt;br/&gt;&amp;lt;dependencyManagement&amp;gt;&#xD;
    &amp;lt;dependencies&amp;gt;&#xD;
        &amp;lt;dependency&amp;gt;&#xD;
            &amp;lt;groupId&amp;gt;org.springframework.cloud&amp;lt;/groupId&amp;gt;&#xD;
            &amp;lt;artifactId&amp;gt;spring-cloud-dependencies&amp;lt;/artifactId&amp;gt;&#xD;
            &amp;lt;version&amp;gt;Hoxton.RELEASE&amp;lt;/version&amp;gt;&#xD;
            &amp;lt;type&amp;gt;pom&amp;lt;/type&amp;gt;&#xD;
            &amp;lt;scope&amp;gt;import&amp;lt;/scope&amp;gt;&#xD;
        &amp;lt;/dependency&amp;gt;&#xD;
    &amp;lt;/dependencies&amp;gt;&#xD;
&amp;lt;/dependencyManagement&amp;gt;&#xD;
&amp;lt;dependencies&amp;gt;&#xD;
    &amp;lt;dependency&amp;gt;&#xD;
        &amp;lt;groupId&amp;gt;org.springframework.cloud&amp;lt;/groupId&amp;gt;&#xD;
        &amp;lt;artifactId&amp;gt;spring-cloud-starter-config&amp;lt;/artifactId&amp;gt;&#xD;
    &amp;lt;/dependency&amp;gt;&#xD;
    &amp;lt;dependency&amp;gt;&#xD;
        &amp;lt;groupId&amp;gt;org.springframework.cloud&amp;lt;/groupId&amp;gt;&#xD;
        &amp;lt;artifactId&amp;gt;spring-cloud-starter-netflix-eureka-client&amp;lt;/artifactId&amp;gt;&#xD;
    &amp;lt;/dependency&amp;gt;&#xD;
    ...&#xD;
&amp;lt;/dependencies&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;or with Gradle:&lt;/p&gt;
&lt;pre&gt;&lt;code class="prettyprint"&gt;buildscript {&#xD;
dependencies {&#xD;
classpath &amp;quot;io.spring.gradle:dependency-management-plugin:1.0.2.RELEASE&amp;quot;&#xD;
}&#xD;
}&#xD;
&#xD;
&#xD;
&#xD;
apply plugin: &amp;quot;io.spring.dependency-management&amp;quot;&#xD;
&#xD;
dependencyManagement {&#xD;
imports {&#xD;
mavenBom &amp;#39;org.springframework.cloud:spring-cloud-dependencies:Hoxton.RELEASE&amp;#39;&#xD;
}&#xD;
}&#xD;
&#xD;
dependencies {&#xD;
compile &amp;#39;org.springframework.cloud:spring-cloud-starter-config&amp;#39;&#xD;
compile &amp;#39;org.springframework.cloud:spring-cloud-starter-netflix-eureka-client&amp;#39;&#xD;
...&#xD;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;!-- rendered by Sagan Renderer Service --&gt;</content>
  </entry>
</feed>
